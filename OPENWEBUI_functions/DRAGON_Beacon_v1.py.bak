"""
description: Beacon is an AI assistant specialized in armed conflict, human rights and international humanitarian law.
requirements: coloredlogs==15.0.1, aiolimiter, instructor, groq==0.21.0, langgraph, langchain-groq==0.2.1, langchain-core==0.3.35, langchain-community==0.3.10, pydantic==2.10.3, fastapi==0.115.6, asyncio, langchain-neo4j==0.3.0, requests==2.32.3, langchain-mistralai==0.2.6, rich, openai, mistralai, beautifulsoup4==4.12.3
"""
from typing import AsyncGenerator
from typing import Any, Awaitable, Callable
from langchain_groq import ChatGroq
from groq import Groq
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from fastapi import Request
import asyncio
import os
import json
import re
import requests
import coloredlogs
from langchain_core.tools import tool
import logging
from langchain_core.messages import (
    AIMessage,
    SystemMessage,
    HumanMessage,
    ToolMessage,
    BaseMessageChunk
)
from langchain_core.messages.utils import convert_to_openai_messages
from langchain_mistralai import ChatMistralAI
# from langchain_ollama import ChatOllama
# from langchain_mistralai.agents import Agents
from rich.console import Console
from rich.panel import Panel
from rich.pretty import Pretty
from rich.text import Text
from rich.markdown import Markdown
from rich.table import Table
import os
from mistralai import Mistral
from openai import OpenAI
from typing import List, Dict
from typing import List, Union, Generator, Iterator
from typing import AsyncGenerator
import sys
from datetime import datetime
from zoneinfo import ZoneInfo
from urllib.parse import urlparse, urljoin
import time

# Load Beacon public code files either in mounted volume (when running normally) or just from local files (when running tests)
if os.path.exists("/app/backend/beacon_code"):
    # Ensure that the mounted volume is in sys.path so that the beacon_code package can be found.
    sys.path.insert(0, "/app/backend/beacon_code")
    print("DEBUG: Updated sys.path:", sys.path)

# Load Beacon public files from either local files or on server if os path exists
try:
    # load tools / functions
    from tools.RULAC_tools import get_armed_conflict_data_by_country
    from tools.RULAC_tools import get_armed_conflict_data_by_non_state_actor
    from tools.RULAC_tools import get_armed_conflict_data_by_organization
    from tools.RULAC_tools import get_armed_conflict_data_by_region
    from tools.RULAC_tools import get_information_about_RULAC
    from tools.RULAC_tools import get_RULAC_conflict_classification_methodology
    from tools.RULAC_tools import get_international_law_framework
    from tools.RULAC_tools import get_information_about_Beacon
    # from tools.WEB_tools import get_website  # Temporarily disabled
    from tools.BRAVE_tools import brave_search  # New Brave Search tool
    from tools.HRW_tools import get_human_rights_research_by_country

    from tools.NEWS_tools import get_combined_news


    # load Final System Prompts for General and Tool Agent
    from prompts.final_prompts import final_beacon_base_model_prompt
    from prompts.final_prompts import final_tool_prompt

    # Import our new Router from agents folder
    from agents.router import Router, RouterResponse

except Exception as e:
    print("DEBUG: Error importing beacon files:", e)
    raise


# // LOGGING //

# Configure logging (should be set to INFO unless DEBUG needed)
logger = logging.getLogger("Beacon")
coloredlogs.install(
    logger=logger,
    level="INFO",
    isatty=True,
    fmt="%(asctime)s [%(levelname)s] %(message)s",
)
# Initialize Rich Console
console = Console()

# Add new debug helpers for API calls
def log_api_request(provider, model, params=None):
    """Log details about an API request"""
    logger.debug(f"API Request to {provider} - Model: {model}")
    if params and logger.isEnabledFor(logging.DEBUG):
        sanitized_params = {k: v for k, v in params.items() if k not in ['messages']}
        if 'messages' in params:
            sanitized_params['message_count'] = len(params['messages'])
        console.print(Panel(
            Pretty(sanitized_params),
            title=f"[bold]{provider} Request Parameters[/bold]",
            border_style="blue",
            expand=False
        ))

def log_api_response_time(provider, model, start_time, status="success"):
    """Log the time taken for an API call"""
    duration = time.time() - start_time
    color = "green" if status == "success" else "red"
    logger.info(f"API Response from {provider} ({model}) - Time: {duration:.2f}s - Status: {status}")
    if duration > 5:
        logger.warning(f"Slow API response from {provider} ({model}): {duration:.2f}s")

# Helper function to log all messages being sent to an LLM
def log_final_messages(messages, title="Messages to LLM"):
    """
    Display all messages being sent to an LLM in a nicely formatted panel.
    
    Args:
        messages (list): List of messages (can be LangChain message objects or dicts with role/content)
        title (str, optional): The title for the panel. Defaults to "Messages to LLM".
    """
    # Create a prettier title with a timestamp
    timestamp = datetime.now().strftime("%H:%M:%S")
    formatted_title = f"[bold magenta]{title}[/] [dim]({timestamp})[/]"
    
    # Format messages for display
    formatted_text = Text()
    
    # Count messages by role for summary
    role_counts = {}
    
    for i, msg in enumerate(messages):
        # Handle different message formats (LangChain objects vs dicts)
        if isinstance(msg, dict):
            role = msg.get("role", "unknown").upper()
            content = msg.get("content", "")
        elif hasattr(msg, "type") and hasattr(msg, "content"):
            # LangChain message objects
            role = msg.type.upper()
            content = msg.content
        else:
            # Fallback for other object types
            role = type(msg).__name__.upper()
            content = str(msg)
            
        # Update role counts
        role_counts[role] = role_counts.get(role, 0) + 1
        
        # Format the message
        formatted_text.append(f"\n[{i+1}] ", style="bold")
        
        # Color-code by role
        if role == "SYSTEM" or role == "SYSTEMMESSAGE":
            formatted_text.append(f"{role}: ", style="bold red")
            # Truncate long system messages
            if len(content) > 200:
                preview = content[:200].replace('\n', ' ').replace('  ', ' ')
                formatted_text.append(f"{preview}... [truncated, {len(content)} chars]\n")
            else:
                formatted_text.append(f"{content}\n")
        elif role == "USER" or role == "HUMANMESSAGE":
            formatted_text.append(f"{role}: ", style="bold blue")
            formatted_text.append(f"{content}\n")
        elif role == "ASSISTANT" or role == "AIMESSAGE":
            formatted_text.append(f"{role}: ", style="bold green")
            formatted_text.append(f"{content}\n")
        elif role == "TOOL" or role == "TOOLMESSAGE":
            formatted_text.append(f"{role}: ", style="bold yellow")
            # Truncate long tool outputs
            if len(content) > 200:  # Truncate long tool outputs
                preview = content[:200].replace('\n', ' ').replace('  ', ' ')
                formatted_text.append(f"{preview}... [truncated, {len(content)} chars]\n")
            else:
                formatted_text.append(f"{content}\n")
        else:
            formatted_text.append(f"{role}: ", style="bold")
            formatted_text.append(f"{content}\n")
    
    # Create summary of message counts
    summary = " | ".join([f"{role}: {count}" for role, count in role_counts.items()])
    logger.debug(f"Logging {len(messages)} messages: {summary}")
    
    # Add summary to the top
    summary_text = Text(f"Total: {len(messages)} messages ({summary})\n", style="bold underline")
    formatted_text = Text.assemble(summary_text, formatted_text)
    
    # Display in a panel
    panel = Panel(
        formatted_text,
        style="white on black",
        border_style="magenta",
        title=formatted_title,
        title_align="left",
        expand=False,
        width=120  # Limit width for better readability
    )
    console.print(panel)

# // TYPE DEFINITIONS //

class User(BaseModel):
    """Available User data from OPENWEBUI"""
    id: str
    email: str
    name: str
    role: str


# // HELPER FUNCTIONS //

def load_template_from_path(file_path: str) -> str:
    """
    Load template text from a local file path.
    """
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read()

def generate_title(body: dict) -> str:
    """Generate a concise title using Groq LLM based on the user's query"""
    try:
        # Initialize Groq client
        chat_client = Groq(api_key="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")
        
        # Get the user's message
        messages = body.get("messages", [])
        if not messages:
            return json.dumps({"title": "New Chat"})
            
        # Format messages for the LLM
        completion = chat_client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": messages[0].get("content", "")}],
            temperature=0.1,
            max_tokens=100,  # Keep it concise
            stream=False
        )
        
        # Extract the title from the response
        response = completion.choices[0].message.content.strip()
        
        # Try to parse as JSON if it's already in JSON format
        try:
            json_response = json.loads(response)
            if isinstance(json_response, dict) and "title" in json_response:
                return json.dumps(json_response)
        except json.JSONDecodeError:
            pass
            
        # If not JSON, clean the response and format it as JSON
        # Remove any quotes or special characters
        title = response.replace('"', '').replace("'", "").strip()
        # Limit length and wrap in JSON format
        title = title[:50]  # Limit length
        
        return json.dumps({"title": title})
        
    except Exception as e:
        print(f"Error generating title: {e}")
        return json.dumps({"title": "New Chat"})

# // PIPELINE //

class Pipe:

    # Setup user-configurable Valves providing API keys and access
    class Valves(BaseModel):
        MODEL_ID: str = Field(default="beacon", description="Model ID for the Beacon project")
        groq_api_key: str = Field("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", description="API key for Groq")
        mistral_api_key: str = Field("9hblEwepQtzvyY9y4incc3yvApk4ArJO", description="API key for Mistral")
        deepseek_api_key: str = Field("sk-28cf4f690b704c76b3f3c6622d7b87cd", description="API key for Deepseek")
        
        # Web search tool parameters
        searxng_url: str = Field("http://localhost:8081/search", description="SearXNG API URL")
        ignored_websites: str = Field("", description="Comma-separated list of websites to ignore in search results")
        page_content_words_limit: int = Field(5000, description="Limit words content for each page")

        

    # Initialize pipeline (ie. once at start) to define the neo4j URL and the LLMs to use
    def __init__(self, local_testing: bool = False):

        self.name = "Beacon"

        self.valves = self.Valves()
        self.local_testing = local_testing  # Set local testing flag
        # Initialize global variables for RULAC citations
        self.global_unique_citations_retreived = set()
        self.global_citation_count = 0  # Reset the citation count at the start of each pipe run
        self.event_emitter = None  # Will be set in pipe() method
        
        # Not sure if i need this, but openwebui tools docs say to set citation to False if you want to customise the citation event ... wont hurt to keep just in case
        self.citation = False

        # For local testing, create a mock event emitter that displays events in the console
        if self.local_testing:
            self.mock_event_emitter = self.create_mock_event_emitter()
        
        # // Initialize all LLMs using API key and models //

        # 1. Router LLM (in order of pref)
        # Initialize the router directly - it now creates its own groq client with instructor
        self.router = Router()
        # Set the API key from valves
        self.router.groq_api_key = self.valves.groq_api_key

        # 2. General LLM (in order of pref)
        self.general_model = ChatGroq(groq_api_key=self.valves.groq_api_key, model="llama3-70b-8192", temperature=0, streaming=True) # Basic start w fast and general intellgent llama3 70B, TO DO: switch to Mistral


        # 3. Tool-Use LLM (in order of pref) - Must be a LLM that features tool use (llama3, etc.)
        self.tool_model = ChatGroq(groq_api_key=self.valves.groq_api_key, model="llama-3.3-70b-versatile", temperature=0) # for now, very fast and intelligent enough... may run into context size issues depending on how much research is retreived from tools

        # self.tool_model = ChatGroq(groq_api_key=self.valves.groq_api_key, model="llama-3.1-8b-instant", temperature=0) # ultra fast with higher context window but not very intelligent and can get confused with too much data
        
        # Bind tools to tool model, for now, only RULAC tool, can add new tools later (HRW, ICRC, etc.)
        self.tools = [
            # RULAC tools
            get_armed_conflict_data_by_country,
            get_armed_conflict_data_by_non_state_actor,
            get_armed_conflict_data_by_organization,
            get_armed_conflict_data_by_region,
            get_information_about_RULAC,
            get_RULAC_conflict_classification_methodology,
            get_international_law_framework,
            get_information_about_Beacon,
            # Web search tools
            brave_search,  # Added Brave Search tool
            # get_website,  # Temporarily disabled
            # Human Rights tools
            get_human_rights_research_by_country,
            # News tools
            get_combined_news
        ]
        
        self.tool_model_with_tools = self.tool_model.bind_tools(self.tools)
        
        # Add tool-friendly names for UI status updates
        self.tool_friendly_names = {
            "get_armed_conflict_data_by_country": "üîç  Researching state actor involvement in armed conflicts",
            "get_armed_conflict_data_by_non_state_actor": "üîç  Researching non-state actor involvement in armed conflicts",
            "get_armed_conflict_data_by_organization": "üîç  Researching organizational involvement in armed conflicts",
            "get_armed_conflict_data_by_region": "üîç  Researching regional involvement in armed conflicts",
            "get_information_about_RULAC": "‚ÑπÔ∏è  Retrieving information about RULAC",
            "get_RULAC_conflict_classification_methodology": "‚öñÔ∏è  Retrieving conflict classification methodology",
            "get_international_law_framework": "‚öñÔ∏è  Retrieving international law framework",
            "get_information_about_Beacon": "‚ÑπÔ∏è  Retrieving information about Beacon",
            "get_human_rights_research_by_country": "üîç  Researching human rights situation",
            "get_combined_news": "üìö  Retrieving latest news and developments",
            "brave_search": "üîé  Searching the web for information"
        }
        
    def create_mock_event_emitter(self):
        """Creates a mock event emitter for local testing that displays events in the console

        The event emitter expects events in the following format:
        For citations:
        {
            "type": "citation",
            "data": {
                "document": [str],  # List of document content strings
                "metadata": [{      # List of metadata dicts
                    "date_accessed": str,  # ISO format datetime
                    "source": str   # Title/name of source
                }],
                "source": {
                    "name": str,    # URL or source name
                    "url": str      # Full URL to source
                }
            }
        }
        
        For status updates:
        {
            "type": "status", 
            "data": {
                "description": str,  # Status message to display
                "done": bool        # Whether this is the final status
            }
        }

        """
        async def mock_event_emitter(event):
            event_type = event.get("type", "unknown")
            if event_type == "status":
                description = event.get("data", {}).get("description", "No description available")
                panel = Panel.fit(description, style="black on yellow", border_style="yellow")
                console.print(panel)
            # elif event_type == "citation":
            #     # Print the citation event, but only if debug logging is enabled
            #     # if logger.isEnabledFor(logging.DEBUG):
            #     #     logger.debug(f"Citation event received: {event}")
            #     data = event.get("data", {})
            #     source = data.get("source", {})
            #     source_url = source.get("url", "No URL")
            #     metadata = data.get("metadata", [])
                
            #     # Get the title from the metadata - metadata is a list of dictionaries
            #     displayed_source_title = metadata[0].get("source", "Unknown Source") if metadata else "Unknown Source"
            #     # Get document content (first item only for display purposes)
            #     documents = data.get("document", [])
            #     document_preview = documents[0][:200] + "..." if documents and len(documents[0]) > 200 else "No content"
                
            #     # Format the URL for display purposes
            #     parsed_url = urlparse(source_url)
            #     display_url = f"{parsed_url.netloc}{parsed_url.path}"

            #     # Create a citation panel with more information
            #     citation_text = Text()
            #     citation_text.append(f"UI Citation: {displayed_source_title}\n", style="bold")
            #     citation_text.append(f"{display_url}\n", style="underline blue")
            #     citation_text.append(f"Preview: {document_preview}", style="italic")
                

            #     panel = Panel(
            #         citation_text,
            #         style="white on green",
            #         border_style="green", 
            #         title="[CITATION]",
            #         title_align="left"
            #     )
            #     console.print(panel)
        return mock_event_emitter
        
    async def emit_event(self, event):
        """Emit an event using the appropriate event emitter"""
        if self.event_emitter:
            await self.event_emitter(event)
        elif self.local_testing and self.mock_event_emitter:
            await self.mock_event_emitter(event)
            
        
    # Main execution logic that starts and ends full pipeline, messages are sent as 'body' input from the OPENWEBUI app and/or through local testing
    async def pipe(
        self,
        body: dict,
        __user__: dict,
        __request__: Request,
        __event_emitter__=None,
        local_testing: bool = False,
        __task__: str = None
    ) -> Union[str, Generator, Iterator, AsyncGenerator]:
        """
        Main orchestration function that processes user requests and returns AI responses.
        
        This function serves as the central pipeline that:
        1. Analyzes the user's request intent
        2. Routes to specialized agents (research or general)
        3. Performs research when needed
        4. Generates and streams a response
        5. Tracks and displays citation information
        
        Args:
            body: Dictionary containing conversation messages
            __user__: User information from OpenWebUI
            __request__: FastAPI request object
            __event_emitter__: Function for sending UI updates
            local_testing: Whether running in local test mode
            __task__: Task to be performed, currently only "title_generation" is supported
            
        Returns:
            Streaming response generator that yields tokens to the user
        """
# This pipe is the main pipeline for the Beacon project, it is used to process user requests and return AI responses.
# Afterwards, it also used to generate titles for the chat history

# If the body contains a request to generate a title, the pipe will return a title
# Otherwise, it will return a streaming response

        # print(f"REQUEST: {__request__}")
# output the body for debugging purposes
        print(f"TASK: {__task__}")
        print("BODY:", body)
# example of body with title generation request
# BODY: {'model': 'beacon_agent_v1_mar_2025', 'messages': [{'role': 'user', 'content': 'Here is the query:\nWhat is the human rights situation in Somalia?\n\nCreate a concise, 3-5 word phrase as a title for the previous query. Avoid quotation marks or special formatting. RESPOND ONLY WITH THE TITLE TEXT.\n\nExamples of titles:\nUkraine Conflict Classification\nUganda Press Freedom\nRacial Justice in USA\nEU Judicial Reform\nLGBTQ+ Rights in China'}], 'stream': False, 'max_completion_tokens': 1000}


        # # check if the body contains a request to generate a title, as they always start with "Here is the query:"  
        # if "Here is the query:" in body.get("messages", [{}])[0].get("content", ""):
        #     print("Title Generation Method 1: Using 'in' operator")
        #     # call the generate_title function
        #     title = generate_title(body)
        #     print(f"\nGenerated Title: {title}")
        #     # return the title and exit the pipeline immediately
        #     return title

        # check if task is title generation
        if __task__ == "title_generation":
            print("Title Generation Method detected: Using task")
            # call the generate_title function
            title = generate_title(body)
            print(f"\nGenerated Title: {title}")
            # return the title and exit the pipeline immediately
            return title
        else:
            print("Title Generation Method not detected: Continuing with pipeline")

        # return "Test Output"




        # Convert __user__ to User type in OpenWebUI
        user = User(**__user__)

        # Save the event emitter for use throughout the pipe execution
        self.event_emitter = __event_emitter__
        
        # If no event emitter is provided, use the mock event emitter for local testing
        if not self.event_emitter:
            self.event_emitter = self.create_mock_event_emitter()

        # Reset citation tracking variables at start of new pipe run
        self.global_unique_citations_retreived = set()
        self.global_citation_count = 0


        # STEP 0: Analyze User Request and determine which LLM to use (generalist LLM or tool-enabled for research LLM)
        # Print clear start marker for pipeline execution
        console.print("\n[bold green]" + "="*50 + "\n" + 
                     "STARTING NEW BEACON PIPELINE EXECUTION\n" + 
                     "="*50 + "[/bold green]\n")

        # Register end marker to be printed at end of execution
        def cleanup():
            console.print("\n[bold green]" + "="*50 + "\n" +
                         "END OF BEACON PIPELINE EXECUTION\n" + 
                         "="*50 + "[/bold green]\n")
            
        # Register cleanup to run at end of pipe() function
        try:
            import atexit
            atexit.register(cleanup)
        except Exception as e:
            logger.debug(f"Could not register cleanup: {e}")

        # Extract messages from body input
        # logger.debug("Full conversation data input:\n%s", json.dumps(body, indent=2))
        messages = body.get("messages", [])
        # logger.debug("All messages extracted from conversation:\n%s", json.dumps(messages, indent=2))

        # Remove any system message from the conversation, for now, as OpenWEBUI adds the date in a system message
        # In the future, we can use the system message to provide context to the router, like USER data and history 
        messages = [msg for msg in messages if msg.get("role") != "system"]

        # STEP 1: Send initial status update to UI
        eventMessageUI = "üí° Analyzing request"
        await self.emit_event(
            {
                "type": "status",
                "data": {
                    "description": eventMessageUI,
                    "done": False,
                },
            }
        )

        # STEP 2: Call Router to analyze user intent and get routing decision, sending in all conversation messages for full context
        # Get model name for logging - use the model name from the router or a default
        router_model_name = self.router.model_name if hasattr(self.router, 'model_name') else "gemma2-9b-it"
        log_api_request("Router", router_model_name, {
            "message_count": len(messages),
            "prompt_tokens_estimate": sum(len(m.get("content", "")) for m in messages) // 4
        })
        
        start_time = time.time()
        logger.info(f"Invoking Router LLM ({router_model_name})")
        
        try:
            router_result = await self.router.route(messages)
            log_api_response_time("Router", router_model_name, start_time)
        except Exception as router_e:
            log_api_response_time("Router", router_model_name, start_time, status="error")
            logger.error(f"Router error: {router_e}")
            # Default to research agent if router fails
            logger.warning("Router failed, defaulting to research agent")
            router_decision = "RESEARCH_AGENT"
            rewritten_user_task = messages[-1].get("content", "") if messages else ""
            conversation_context = ""
            reasoning = "Router failed with error, using default routing"
            news_search_by_topic = ""
        else:
            # Extract routing information from router response
            router_decision = router_result.router_decision      # Classification (RESEARCH_AGENT or BEACON_BASE_AGENT)
            rewritten_user_task = router_result.task             # Rewritten/clarified user query
            conversation_context = router_result.conversational_context  # Context from conversation history
            reasoning = router_result.reasoning                  # Router's reasoning for decision
            news_search_by_topic = router_result.news_search_by_topic  # Query for news search

        # STEP 3: Route to the appropriate agent based on the router's decision
        if router_decision == "RESEARCH_AGENT":
            logger.info("Routing to research agent with tool capabilities")
            # Execute research tools and get their outputs
            tool_outputs = await self.handle_tool_query(rewritten_user_task, conversation_context, news_search_by_topic)
            logger.debug(f"Received {len(tool_outputs)} tool outputs from handle_tool_query")
            
            # Generate final response using tool outputs and original messages
            result = self.handle_tool_query_final_response(tool_outputs, messages)
            
            # Create status message showing citation count, overall from all tools, for display in the UI
            unique_citation_count = len(self.global_unique_citations_retreived)
            print(f"DEBUG: Total unique citations: {unique_citation_count}")

            if unique_citation_count:
                eventMessageUI = f"üåê  Cross-referenced w {unique_citation_count} source{'s' if unique_citation_count != 1 else ''}"
            else:
                # eventMessageUI = "üí°  Beacon enhanced"
                eventMessageUI = " "
        elif router_decision == "BEACON_BASE_AGENT":
            logger.info("Routing to Beacon base agent for general queries")
            # Handle simpler queries that don't need research tools
            result = self.handle_general_query(messages)
            # eventMessageUI = "üí°  Beacon enhanced"
            eventMessageUI = " "
        else:  # Default routing catchall to research agent
            logger.warning(f"Unrecognized router decision: {router_decision}, defaulting to research agent")
            tool_outputs = await self.handle_tool_query(rewritten_user_task, conversation_context, news_search_by_topic)
            result = self.handle_tool_query_final_response(tool_outputs, messages)

        # STEP 4: Send final status update to UI
        if eventMessageUI:
            await self.emit_event(
                {
                    "type": "status",
                    "data": {
                        "description": eventMessageUI,
                        "done": True, # This is a final status event
                    },
                }
            )
            
        # Return streaming response generator to OpenWebUI
        return result

    def handle_general_query(self, conversation_messages: list[dict]) -> Generator[str, None, None]:
        """
        Handles general queries that don't require research tools.
        
        This function is used when the router determines that the user's query
        can be answered directly without utilizing specialized research tools.
        It formats messages with a system prompt and streams a response from
        the general model.
        
        Args:
            conversation_messages: List of conversation messages from the user
            
        Returns:
            Generator that yields streaming tokens as they are produced
        """
        # Debug shape of conversation messages
        # logger.debug(f"Actual Conversation Messages passed in: {conversation_messages}")

        # STEP 1: Set up the system prompt with general Beacon capabilities
        FINAL_SYSTEM_PROMPT = final_beacon_base_model_prompt.PROMPT

        # Add current date/time for temporal context
        currentDateTime = datetime.now(ZoneInfo("Europe/Paris")).strftime("%B %d, %Y %I:%M %p")

        # Create a PromptTemplate and render it with variables
        FINAL_PROMPT = PromptTemplate(
                input_variables=["currentDateTime"],
                template=FINAL_SYSTEM_PROMPT,
            )

        rendered_prompt = FINAL_PROMPT.format(
                currentDateTime=currentDateTime,
            )

        # STEP 2: Prepare messages array starting with system prompt
        system_beacon_and_cleaned_conversation_messages = [
            SystemMessage(content=rendered_prompt),
        ]

        # STEP 3: Convert conversation messages to LangChain format
        for msg in conversation_messages:
            role = msg.get("role", "").lower()
            content = msg.get("content", "")
            if role == "user":
                system_beacon_and_cleaned_conversation_messages.append(HumanMessage(content=content))
            elif role == "assistant":
                system_beacon_and_cleaned_conversation_messages.append(AIMessage(content=content))
            elif role == "system":
                # In case system messages are passed in the conversation
                system_beacon_and_cleaned_conversation_messages.append(SystemMessage(content=content))
            else:
                # Skip unrecognized roles
                logger.debug(f"Unrecognized role '{role}' in message; skipping.")

        # Log final messages for debugging
        log_final_messages(system_beacon_and_cleaned_conversation_messages, "Beacon Base Agent Complete Messages")

        logger.debug("Inside general query final response; starting synchronous streaming...")

        # STEP 4: Stream response from the general model
        try:
            # Get streaming generator from the general model
            stream = self.general_model.stream(system_beacon_and_cleaned_conversation_messages, stop=None)
            for i, chunk in enumerate(stream):
                if hasattr(chunk, "content"):
                    yield chunk.content
                else:
                    logger.debug(f"Chunk {i} has no 'content' attribute.")
            logger.debug("Finished processing all chunks.")
        except Exception as e:
            logger.error(f"Streaming invocation failed: {e}")
            yield f"Error: {e}"


    async def handle_tool_query(self, router_task: str, router_conversation_context: str, news_search_by_topic: str) -> list[dict]:
        """
        Research orchestration function that selects and executes appropriate research tools.
        
        This function:
        1. Constructs prompts for the tool-using LLM
        2. Gets tool selection decisions from the LLM
        3. Executes selected research tools (RULAC, HRW, etc.)
        4. Processes and emits citation information to the UI
        5. Collects all tool outputs for final response generation
        
        Args:
            router_task: The rewritten/clarified user task from the router
            router_conversation_context: Relevant conversation context from prior messages
            news_search_by_topic: Query for news search functionality
            
        Returns:
            List of dictionaries containing tool outputs with metadata
        """

        console.print("\n[bold yellow]" + "="*50 + "\n" +
                         "START OF TOOL USE AGENT\n" + 
                         "="*50 + "[/bold yellow]\n")

        # Initialize data structures for tool processing
        messages = []
        tool_outputs = []  # List to store tool outputs
        
        # Add the router's rewritten task as a user message
        messages.append(HumanMessage(content=router_task))

        # STEP 1: Start news search early if we have a search query
        news_search_future = None
        news_tool_id = "early_get_combined_news"
        if news_search_by_topic and news_search_by_topic.strip():
            logger.info(f"Starting early news search with query: {news_search_by_topic}")
            
            # Start news search in background but don't await it yet
            news_search_future = asyncio.create_task(
                get_combined_news.ainvoke({"search_query": news_search_by_topic})
            )

        # STEP 2: Prepare the system prompt for the tool-using LLM
        system_message_content = (
            """You are a helpful expert in human rights, armed conflict and international humanitarian law named "Beacon". 
            You answer questions and complete tasks about human rights, conflict and international humanitarian law by using tools that provide you with research.

            AVAILABLE TOOLS:

            1. RULAC (Rule of Law in Armed Conflicts) Tools:
               Use these tools to get detailed information about armed conflicts and their legal classification.

               a) State Actor Conflicts:
                  - Tool: get_armed_conflict_data_by_country
                  - Use when: You need information about conflicts involving specific countries
                  - Parameters: 
                    * countries: List of country names
                    * conflict_types: Optional list of ["International Armed Conflict (IAC)", "Non-International Armed Conflict (NIAC)", "Military Occupation"]

               b) Non-State Actor Conflicts:
                  - Tool: get_armed_conflict_data_by_non_state_actor
                  - Use when: You need information about conflicts involving non-state actors
                  - Parameters:
                    * non_state_actors: List of actor names/aliases

               c) Regional Conflicts:
                  - Tool: get_armed_conflict_data_by_region
                  - Use when: You need information about conflicts in specific regions
                  - Parameters:
                    * regions: List of region names (e.g., "Eastern Africa", "Europe")
                    * conflict_types: Optional list of conflict types

               d) Organization Conflicts:
                  - Tool: get_armed_conflict_data_by_organization
                  - Use when: You need information about conflicts involving specific organizations
                  - Parameters:
                    * organizations: List of organization names (e.g., "NATO", "BRICS")
                    * conflict_types: Optional list of conflict types

               e) General RULAC Information:
                  - Tool: get_information_about_RULAC
                  - Use when: You need general information about the RULAC project
                  - No parameters required

               f) Conflict Classification:
                  - Tool: get_RULAC_conflict_classification_methodology
                  - Use when: You need information about how RULAC classifies conflicts
                  - No parameters required


               g) International Law Framework:
                  - Tool: get_international_law_framework
                  - Use when: You need information about international legal frameworks (IHL, IHR, ICL)
                  - Parameters:
                    * law_focus: Must be one of: "International Humanitarian Law (IHL)", "International Human Rights Law (IHR)", or "International Criminal Law (ICL)"


            2. Human Rights Tools:
               Use these tools to get information about human rights situations.

               a) HRW Reports:
                  - Tool: get_human_rights_research_by_country
                  - Use when: You need the latest Human Rights Watch report for a country
                  - Parameters:
                    * country: The country name
                  - Note: Use this tool whenever a country is mentioned, in addition to other relevant tools

            3. Web Search Tools:
               Use these tools to get current information and answers to factual questions.

               a) Brave Search:
                  - Tool: brave_search
                  - Use when: You need to search for factual information or get summarized answers to questions
                  - Parameters:
                    * query: Your search query. When using this tool, you should break down complex queries into separate focused searches
                  - Note: This tool is useful for getting current information about topics not covered by RULAC or HRW
                  - Note: When using this tool, you should break down complex queries into separate focused searches, and use the tool multiple times if needed

            4. Beacon Tools:
               Use these tools to get information about Beacon.

               a) Beacon Information:
                  - Tool: get_information_about_Beacon
                  - Use when: You need information about Beacon, or ask about your purpose, capabilities, or tools
                  - No parameters required
                  
            IMPORTANT NOTES:
            - If you use the get_beacon_information tool, it MUST be the only tool call in your response
            - You can use multiple tools in a single response
            - Prioritize using the RULAC tools for conflict information
            - For RULAC tools, use empty lists [] for optional parameters to get all results
            - Always consider using get_human_rights_research_by_country when countries are mentioned
            - Use brave_search when you need factual information not covered by other specialized tools
            - If you use get_RULAC_conflict_classification_methodology, you must also use get_armed_conflict_data_by_country or get_armed_conflict_data_by_non_state_actor
            - Return your response as a JSON object with the selected tool calls and their arguments
            """
        )

        # Add conversation context to system message if available
        if router_conversation_context:
            system_message_content = f"Conversation Context:\n{router_conversation_context}\n\n{system_message_content}"
            
        # Add system message to beginning of messages array
        messages.insert(0, SystemMessage(content=system_message_content))

        # Log messages being sent to the tool model for debugging
        log_final_messages(messages, "Final Prompt sent to Tool LLM")

        try:
            # STEP 3: Get tool selection decisions from the LLM
            # Send the prompt to the tool-using LLM and require it to select tools
            model_name = self.tool_model.model if hasattr(self.tool_model, 'model') else "llama-3.3-70b-versatile"
            log_api_request("Tool Model", model_name, {
                "tool_choice": "required",
                "message_count": len(messages),
                "prompt_tokens_estimate": sum(len(msg.content) if hasattr(msg, 'content') else 100 for msg in messages) // 4
            })
            
            start_time = time.time()
            logger.info(f"Invoking Tool Selection LLM ({model_name})")
            
            try:
                ai_msg = self.tool_model_with_tools.invoke(messages, tool_choice="required")
                log_api_response_time("Tool Model", model_name, start_time)
                
                # Log raw tool calls structure for debugging
                logger.debug(f"Raw tool_calls structure from LLM: {json.dumps(ai_msg.tool_calls, indent=2)}")
            except Exception as tool_e:
                log_api_response_time("Tool Model", model_name, start_time, status="error")
                error_type = type(tool_e).__name__
                error_details = str(tool_e)
                
                # Log detailed error information
                logger.error(f"Tool Model API Error ({error_type}): {error_details}")
                
                # Create a detailed error panel for inspection
                error_panel = Panel(
                    f"Error Type: {error_type}\nDetails: {error_details}",
                    title="Tool Model LLM API Error",
                    border_style="red",
                    expand=False
                )
                console.print(error_panel)
                
                # Re-raise the exception to be caught by the outer try/except
                raise

            # STEP 4: Check if we need to manually add the news search
            # Only skip adding news search if there's already a news call or it's a single tool call for Beacon information
            skip_adding_news = False
            news_already_requested = any(
                tool_call['name'] == "get_combined_news" 
                for tool_call in ai_msg.tool_calls
            )
            
            # Skip if it's just a Beacon information request or news is already requested
            if (len(ai_msg.tool_calls) == 1 and ai_msg.tool_calls[0]['name'] == "get_information_about_Beacon") or news_already_requested:
                skip_adding_news = True
                # If we're not using the news search we started earlier, cancel it
                if news_search_future and not news_already_requested:
                    news_search_future.cancel()
                    news_search_future = None
            else:
                # Only emit status update if we're actually going to use the news search
                await self.emit_event({
                    "type": "status",
                    "data": {
                        "description": f"üìö  Checking news on '{news_search_by_topic}'",
                        "done": False,
                    }
                })
            
            # If we need to add news search manually and we didn't start it earlier
            if not skip_adding_news and not news_search_future and news_search_by_topic:
                # Add news search tool call
                ai_msg.tool_calls.append({
                    "id": "manual_get_combined_news",
                    "name": "get_combined_news",
                    "args": {"search_query": news_search_by_topic}
                })
            
            # STEP 5: Display tool selection decisions
            formatted_tool_decision = Text()
            formatted_tool_decision.append("Tool Decision Summary\n", style="bold underline")
            formatted_tool_decision.append(f"Tool Calls: {len(ai_msg.tool_calls)}\n", style="bold")
            
            if hasattr(ai_msg, 'tool_calls') and ai_msg.tool_calls:
                for i, tool_call in enumerate(ai_msg.tool_calls, 1):
                    formatted_tool_decision.append(f"\nTool Call {i}:\n", style="bold")
                    formatted_tool_decision.append(f"Name: {tool_call['name']}\n")
                    formatted_tool_decision.append("Arguments:\n")
                    for key, value in tool_call['args'].items():
                        formatted_tool_decision.append(f"  {key}: {value}\n")
            else:
                formatted_tool_decision.append("No tool calls made\n")
                
            panel = Panel(
                formatted_tool_decision,
                style="white on black",
                border_style="yellow",
                title="[TOOL DECISIONS]",
                title_align="left",
                expand=True,
            )
            console.print(panel)

            # # STEP 6: Send UI status update about tool selection
            # eventMessageUI = f"üõ†Ô∏è  {len(ai_msg.tool_calls)} research tool{'' if len(ai_msg.tool_calls) == 1 else 's'} identified"
            # await self.emit_event(
            #     {
            #         "type": "status",
            #         "data": {
            #             "description": eventMessageUI,
            #             "done": False,
            #         },
            #     }
            # )

            console.print("\n[bold yellow]" + "="*50 + "\n" +
                            "START OF TOOL CALLS\n" + 
                            "="*50 + "[/bold yellow]\n")

            # STEP 7: Execute all tools in parallel using asyncio.gather
            async def process_tool_call(tool_call):
                # Skip unavailable tools
                # at one point, brave_search was not working, so we skipped it
                # if tool_call["name"] == "brave_search":
                #     logger.warning(f"Skipping unavailable brave_search tool call with args: {tool_call['args']}")
                #     return None
                
                # Display tool call details for debugging
                formatted_tool_call = Text()
                formatted_tool_call.append(f"Processing Tool Call: {tool_call['name']}\n", style="bold underline")
                formatted_tool_call.append("Arguments:\n")
                for key, value in tool_call['args'].items():
                    formatted_tool_call.append(f"  {key}: {value}\n")
                panel = Panel(
                    formatted_tool_call,
                    style="white on black",
                    border_style="blue",
                    title="[PROCESSING TOOL]",
                    title_align="left",
                    expand=True,
                )
                console.print(panel)

                # Skip the news tool if we're already running it in the background
                if tool_call["name"] == "get_combined_news" and news_search_future:
                    logger.info("Skipping duplicate news search as it's already running in the background")
                    return None
                
                # Select the appropriate tool function based on name
                tool_functions = {
                    "get_armed_conflict_data_by_country": get_armed_conflict_data_by_country,
                    "get_armed_conflict_data_by_non_state_actor": get_armed_conflict_data_by_non_state_actor,
                    "get_armed_conflict_data_by_organization": get_armed_conflict_data_by_organization,
                    "get_armed_conflict_data_by_region": get_armed_conflict_data_by_region,
                    "get_information_about_RULAC": get_information_about_RULAC,
                    "get_RULAC_conflict_classification_methodology": get_RULAC_conflict_classification_methodology,
                    "get_international_law_framework": get_international_law_framework,
                    "get_information_about_Beacon": get_information_about_Beacon,
                    # "get_website": get_website,  # Temporarily disabled
                    "brave_search": brave_search,  # Brave Search tool for web queries
                    "get_human_rights_research_by_country": get_human_rights_research_by_country,
                    "get_combined_news": get_combined_news
                }
                
                if tool_call["name"] not in tool_functions:
                    logger.warning(f"Unknown tool: {tool_call['name']}")
                    return None
                
                selected_tool = tool_functions[tool_call["name"]]
                
                # Get tool-specific friendly name for UI status
                tool_name = tool_call["name"]
                friendly_description = self.tool_friendly_names.get(
                    tool_name, 
                    f"üîç Researching with {tool_name}..."
                )

                # Add detailed tool params to status (optional)
                if tool_name == "get_armed_conflict_data_by_country" and "countries" in tool_call["args"]:
                    countries = tool_call["args"]["countries"]
                    if isinstance(countries, list) and countries:
                        country_names = " and ".join(countries)
                        friendly_description = f"üîé Analyzing RULAC conflict data on {country_names}"
                elif tool_name == "get_human_rights_research_by_country" and "country" in tool_call["args"]:
                    country = tool_call["args"]["country"]
                    friendly_description = f"üîé Analyzing HRW human rights data on {country}"
                elif tool_name == "brave_search" and "query" in tool_call["args"]:
                    query = tool_call["args"]["query"]
                    friendly_description = f"üîé Searching for information on '{query}'"

                # Emit tool-specific status update
                await self.emit_event(
                    {
                        "type": "status",
                        "data": {
                            "description": friendly_description,
                            "done": False,
                        },
                    }
                )

                # Execute the tool and return results
                try:
                    tool_output = await selected_tool.ainvoke(tool_call["args"])
                    
                    # Process the output
                    if isinstance(tool_output, dict) and "content" in tool_output and "citations" in tool_output:
                        content = tool_output["content"]
                        citations = tool_output["citations"]
                        
                        # Process citations
                        if citations and isinstance(citations, list):
                            for citation in citations:
                                if isinstance(citation, dict) and "url" in citation:
                                    citation_url = citation.get("url", "")
                                    citation_title = citation.get("title", "Source")
                                    formatted_content = citation.get("formatted_content", "")
                                    
                                    # Only emit new citations to avoid duplicates
                                    if citation_url and citation_url not in self.global_unique_citations_retreived:
                                        # Create citation event for UI
                                        citation_event = {
                                            "type": "citation",
                                            "data": {
                                                "document": [formatted_content],
                                                "metadata": [
                                                    {
                                                        "date_accessed": datetime.now().isoformat(),
                                                        "source": citation_title,
                                                    }
                                                ],
                                                "source": {
                                                    "name": citation_url, 
                                                    "url": citation_url
                                                },
                                            }
                                        }

                                        # Emit citation event to UI
                                        await self.emit_event(citation_event)
                                        # Track citation count
                                        self.global_citation_count += 1
                                        # Track unique URLs to prevent duplicates
                                        self.global_unique_citations_retreived.add(citation_url)
                        
                        # Return formatted output
                        return {
                            "tool_name": tool_call["name"],
                            "tool_call_id": tool_call["id"],
                            "content": content,
                            "args": tool_call["args"]
                        }
                    else:
                        # Handle non-standard tool outputs
                        return {
                            "tool_name": tool_call["name"],
                            "tool_call_id": tool_call["id"],
                            "content": tool_output,
                            "args": tool_call["args"]
                        }
                except Exception as e:
                    # Handle tool execution errors
                    panel = Panel.fit(
                        f"‚ùå Error invoking tool {tool_call['name']}:\n{e}",
                        style="white on red",
                        border_style="red",
                    )
                    console.print(panel)
                    logger.error(f"Tool execution error: {e}")
                    
                    # Return error information
                    return {
                        "tool_name": tool_call["name"],
                        "tool_call_id": tool_call["id"],
                        "content": f"Error executing tool: {str(e)}",
                        "error": True,
                        "args": tool_call["args"]
                    }

            # Create list of tasks to run in parallel
            tool_tasks = []
            
            # Add news search task if it's already running
            if news_search_future:
                # Create a wrapper task that processes the news search result
                async def process_news_result():
                    try:
                        news_result = await news_search_future
                        logger.info("Early news search completed")
                        
                        # Process the result similarly to other tools
                        content = news_result.get("content", "No news content found")
                        citations = news_result.get("citations", [])
                        
                        # Process citations
                        if citations and isinstance(citations, list):
                            for citation in citations:
                                if isinstance(citation, dict) and "url" in citation:
                                    citation_url = citation.get("url", "")
                                    if citation_url and citation_url not in self.global_unique_citations_retreived:
                                        citation_title = citation.get("title", "News Source")
                                        formatted_content = citation.get("formatted_content", "")
                                        
                                        # Create citation event for UI
                                        citation_event = {
                                            "type": "citation",
                                            "data": {
                                                "document": [formatted_content],
                                                "metadata": [
                                                    {
                                                        "date_accessed": datetime.now().isoformat(),
                                                        "source": citation_title,
                                                    }
                                                ],
                                                "source": {
                                                    "name": citation_url, 
                                                    "url": citation_url
                                                },
                                            }
                                        }

                                        # Emit citation event to UI
                                        await self.emit_event(citation_event)
                                        # Track citation count
                                        self.global_citation_count += 1
                                        # Track unique URLs to prevent duplicates
                                        self.global_unique_citations_retreived.add(citation_url)
                        
                        return {
                            "tool_name": "get_combined_news",
                            "tool_call_id": news_tool_id,
                            "content": content,
                            "args": {"search_query": news_search_by_topic}
                        }
                    except asyncio.CancelledError:
                        logger.info("Early news search was cancelled")
                        return None
                    except Exception as e:
                        logger.error(f"Error processing early news search: {e}")
                        return {
                            "tool_name": "get_combined_news",
                            "tool_call_id": news_tool_id,
                            "content": f"Error retrieving news: {str(e)}",
                            "error": True,
                            "args": {"search_query": news_search_by_topic}
                        }
                
                # Add the news processing task to our tasks list
                tool_tasks.append(process_news_result())
            
            # Add all other tool tasks
            for tool_call in ai_msg.tool_calls:
                # Skip news if we're already running it
                if tool_call["name"] == "get_combined_news" and news_search_future:
                    continue
                tool_tasks.append(process_tool_call(tool_call))
            
            # Execute all tools in parallel
            logger.info(f"Running {len(tool_tasks)} tool tasks in parallel")
            tool_results = await asyncio.gather(*tool_tasks)
            
            # Filter out None results and add to tool_outputs
            for result in tool_results:
                if result is not None:
                    tool_outputs.append(result)

            console.print("\n[bold yellow]" + "="*50 + "\n" +
                            "END OF TOOL CALLS\n" + 
                            "="*50 + "[/bold yellow]\n")

            # Log summary of tool execution
            logger.debug(f"Collected {len(tool_outputs)} tool outputs")
            print(f"DEBUG: Successfully emitted all collected citations: {self.global_citation_count}")

            console.print("\n[bold yellow]" + "="*50 + "\n" +
                            "END OF TOOL USE AGENT\n" + 
                            "="*50 + "[/bold yellow]\n")

            # Return collected tool outputs for final response generation
            return tool_outputs

        except Exception as e:
            # Handle overall process errors
            logger.error(f"Error during tool model execution: {e}")
            # If we started news search, try to get that result at least
            if news_search_future and not news_search_future.cancelled():
                try:
                    news_result = await news_search_future
                    logger.info("Retrieved news despite other errors")
                    return [{
                        "tool_name": "get_combined_news",
                        "tool_call_id": news_tool_id,
                        "content": news_result.get("content", ""),
                        "args": {"search_query": news_search_by_topic}
                    }]
                except Exception as news_e:
                    logger.error(f"Also failed to get news: {news_e}")
            
            return []  # Return empty list on error

    def handle_tool_query_final_response(self, tool_outputs: list[dict], original_messages: list[dict]) -> Generator[str, None, None]:
        """
        Generates the final response after all research tools have been executed.
        
        This function:
        1. Processes and organizes research outputs by category (RULAC, HRW, News, etc.)
        2. Combines all research into a structured format
        3. Creates a comprehensive system prompt with the research results
        4. Sends the prompt and original conversation to the final LLM
        5. Streams the generated response back to the user
        
        Args:
            tool_outputs: List of dictionaries containing tool outputs and metadata
            original_messages: Original conversation messages from the user
            
        Returns:
            Generator that streams the final response tokens
        """
        logger.debug("Inside final tool query response function...")
        logger.debug(f"Processing {len(tool_outputs)} tool outputs with {len(original_messages)} original messages")

        # Debug log the structure of an example tool output
        if tool_outputs:
            example_output = tool_outputs[0]
            logger.debug(f"Example tool output structure: {list(example_output.keys())}")
            logger.debug(f"Example tool name: {example_output.get('tool_name', 'N/A')}")
            content_preview = example_output.get('content', '')[:100] + '...' if len(example_output.get('content', '')) > 100 else example_output.get('content', '')
            logger.debug(f"Example content preview: {content_preview}")

        # STEP 1: Categorize research outputs by source/type
        rulac_outputs = []      # RULAC research data
        hrw_outputs = []        # Human Rights Watch reports
        beacon_outputs = []     # Information about Beacon itself
        web_outputs = []        # General web content
        news_outputs = []       # News content

        # Process each tool output
        for output in tool_outputs:
            # Skip empty outputs
            if not output or not output.get("content"):
                continue
                
            # Get the tool name and content
            tool_name = output.get("tool_name", "unknown")
            content = output.get("content", "")
            
            # Log which tool is being processed
            logger.debug(f"Processing output from tool: {tool_name}")
            
            # Categorize by tool source/type
            tool_source = output.get("tool_source", "unknown").upper()
            
            if any(rulac_tool in tool_name for rulac_tool in ["get_information_about_RULAC", "get_international_law_framework", "get_RULAC_conflict_classification_methodology", "get_armed_conflict_data"]):
                rulac_outputs.append(output)
                logger.debug(f"Added RULAC output: {tool_name}")
            elif tool_name == "get_human_rights_research_by_country":
                hrw_outputs.append(content)
                logger.debug(f"Added HRW output: {tool_name}")
            elif "get_information_about_Beacon" in tool_name:
                beacon_outputs.append(content)
                logger.debug(f"Added Beacon info: {tool_name}")
            elif "get_combined_news" in tool_name:
                news_outputs.append(content)
                logger.debug(f"Added News output: {tool_name}")
            elif "brave_search" in tool_name:
                web_outputs.append(content)
                logger.debug(f"Added Web output: {tool_name}")
            else:
                # Default category for unknown sources
                web_outputs.append(content)
                logger.debug(f"Added to Web (default) category: {tool_name}")
        
        # STEP 2: Combine research outputs with appropriate headers and organization
        combined_research = ""
        
        # Add RULAC outputs in a specific, logical order
        if rulac_outputs:
            # Define preferred order of RULAC tools
            rulac_order = [
                "get_information_about_RULAC",
                "get_international_law_framework",
                "get_RULAC_conflict_classification_methodology",
                "get_armed_conflict_data_by_region",
                "get_armed_conflict_data_by_organization",
                "get_armed_conflict_data_by_non_state_actor",
                "get_armed_conflict_data_by_country"
            ]
            
            # Sort RULAC outputs according to defined order
            ordered_rulac_outputs = []
            for tool_name in rulac_order:
                for output in rulac_outputs:
                    if output["tool_name"] == tool_name:
                        ordered_rulac_outputs.append(output["content"])
            
            # Add any remaining outputs not in the predefined order
            for output in rulac_outputs:
                if output["tool_name"] not in rulac_order:
                    ordered_rulac_outputs.append(output["content"])
            
            # Add RULAC section with header
            combined_research += "<RULAC_research>\n## Rule of Law in Armed Conflict (RULAC) research\n\n" 
            combined_research += "The Rule of Law in Armed Conflict (RULAC) portal provides the the most authoritative classifications of armed conflicts based on the parties involved and the nature of the hostilities."
            combined_research += "\n\n" + "\n\n- - -\n\n".join(ordered_rulac_outputs) 
            combined_research += "\n\n</RULAC_research>"
        
        # Add Human Rights Watch outputs
        if hrw_outputs:
            combined_research += "\n\n<HRW_research>\n## Human Rights Watch (HRW) research\n\n"
            combined_research += "Human Rights Watch (HRW) is a non-governmental organization that monitors human rights violations and abuses around the world."
            combined_research += "\n\n" + "\n\n".join(hrw_outputs)
            combined_research += "\n\n</HRW_research>"

        # Add Web Research outputs
        if web_outputs:
            combined_research += "\n\n<web_research>\n## web research\n\n"
            combined_research += "\n\n".join(web_outputs) 
            combined_research += "\n\n</web_research>"

        # Add Beacon information
        if beacon_outputs:
            combined_research += "\n\n<Beacon_information>\n## beacon information\n\n"
            combined_research += "\n\n".join(beacon_outputs)
            combined_research += "\n\n</Beacon_information>"

        # Add latest news
        if news_outputs:
            combined_research += "\n\n<latest_news_and_developments>\n## latest news and developments\n\n"
            combined_research += "".join(news_outputs)
            combined_research += "\n\n</latest_news_and_developments>"

        # Log summary of combined research
        logger.debug(f"Combined research length: {len(combined_research)} characters")
        logger.info(f"RULAC outputs: {len(rulac_outputs)}, HRW outputs: {len(hrw_outputs)}, News outputs: {len(news_outputs)}, Web outputs: {len(web_outputs)}, Beacon outputs: {len(beacon_outputs)}")

        # STEP 3: Prepare the final system prompt with the research
        currentDateTime = datetime.now(ZoneInfo("Europe/Paris")).strftime("%B %d, %Y %I:%M %p")
        
        # Separate the conversation into two parts: without the last user message and the last user message itself
        conversation_without_last_user_message = ""
        last_user_message = ""
        
        # Extract the last user message and build conversation history
        filtered_messages = []
        for msg in original_messages:
            role = msg.get("role", "").lower()
            content = msg.get("content", "")
            
            # Only include user and assistant messages
            if role in ["user", "assistant"] and content:
                filtered_messages.append({"role": role, "content": content})
        
        # Find the last user message
        for i in range(len(filtered_messages) - 1, -1, -1):
            if filtered_messages[i]["role"] == "user":
                last_user_message = filtered_messages[i]["content"]
                # Remove the last user message from the conversation history
                filtered_messages.pop(i)
                break
        
        # Format the conversation history
        for msg in filtered_messages:
            role_display = "User" if msg["role"] == "user" else "Assistant"
            conversation_without_last_user_message += f"\n\n**{role_display}**: {msg['content']}"
        
        # Check if we have a conversation history and format it properly
        if conversation_without_last_user_message:
            conversation_without_last_user_message = conversation_without_last_user_message.strip()
            # Also adding "Conversation History" header information for prompt
            conversation_without_last_user_message = f"--- \n\n# Our current conversation\n\nWe are currently having the following conversation:\n\n{conversation_without_last_user_message}"
       
        # Check if we have a last user message
        if not last_user_message:
            last_user_message = "Please provide information about the current international conflicts."
        
        # STEP 3: Choose the correct prompt template based on the model
        model_name = "deepseek-r1-distill-qwen-32b"
        
        # Import the new Deepseek-specific prompt template
        import prompts.final_prompts.final_tool_prompt_for_Deepseek as final_tool_prompt_deepseek
        DEEPSEEK_PROMPT_TEMPLATE = final_tool_prompt_deepseek.PROMPT
        
        # Render the Deepseek-specific prompt with the required variables
        rendered_system_prompt = DEEPSEEK_PROMPT_TEMPLATE.replace(
            "{currentDateTime}", currentDateTime
        ).replace(
            "{combined_tool_research}", combined_research
        ).replace(
            "{conversation_without_last_user_message}", conversation_without_last_user_message
        ).replace(
            "{last_user_message}", last_user_message
        )
        
        # Display debug information about prompt
        # logger.info(f"System prompt template length: {len(DEEPSEEK_PROMPT_TEMPLATE)}")
        logger.info(f"Final system prompt length: {len(rendered_system_prompt)} characters")
        
        if logger.isEnabledFor(logging.INFO):
            # Truncate the system prompt for debug display
            debuggingSystemPrompt = rendered_system_prompt[:1000]
            # Display the system prompt in a rich panel
            panel = Panel(
                Markdown(debuggingSystemPrompt),
                style="white on black",
                border_style="cyan",
                title="[bold]System Prompt Preview[/bold]",
                title_align="left",
                width=120
            )
            console.print(panel)
      
        # STEP 4: Create the final messages for the LLM
        # For Deepseek, we need to package everything into a single user message
        final_messages = [
            {"role": "user", "content": rendered_system_prompt}
        ]
        
        # Log final prompt with all messages
        # log_final_messages(final_messages, "Research Agent Complete Messages")
        
        # STEP 5: Set up the LLM client for generating the final response
        chat_client = Groq(api_key=self.valves.groq_api_key)
        logger.debug("Starting final tool use LLM response synchronous streaming...")

        # STEP 6: Set up the LLM client for generating and streaming the final response
        # Add detailed tracking of API call
        api_params = {
            "model": model_name,
            "temperature": 0.6,
            "max_completion_tokens": 4096,
            "top_p": 0.95,
            "stream": True,
            "reasoning_format": "raw",
            "message_count": len(final_messages),
            "prompt_tokens_estimate": sum(len(m.get("content", "")) for m in final_messages) // 4
        }
        
        log_api_request("Groq", model_name, api_params)

        # STEP 6: Stream the final response from the LLM with improved error handling and retries
        max_retries = 3
        retry_count = 0
        backoff_time = 1  # Initial backoff in seconds
        
        while retry_count <= max_retries:
            try:
                start_time = time.time()
                logger.info(f"Invoking Groq LLM ({model_name}) - Attempt {retry_count + 1}/{max_retries + 1}")
                
                completion = chat_client.chat.completions.create(
                    model=model_name,
                    messages=final_messages,
                    temperature=0.6,
                    max_completion_tokens=4096,
                    top_p=0.95,
                    stream=True,
                    reasoning_format="raw"
                )
                
                # Process streaming chunks and yield to caller
                chunk_count = 0
                first_token_time = None
                
                for chunk in completion:
                    chunk_count += 1
                    
                    # Record time to first token
                    if chunk_count == 1:
                        first_token_time = time.time()
                        time_to_first_token = first_token_time - start_time
                        logger.info(f"Time to first token: {time_to_first_token:.2f}s")
                    
                    # Log chunk progress periodically
                    if chunk_count % 10 == 0:
                        # logger.debug(f"Processing chunk {chunk_count}...")
                        pass
                    
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                        pass
                    else:
                        logger.debug(f"Chunk {chunk_count} has no content.")
                
                # Log successful completion metrics
                completion_time = time.time() - start_time
                log_api_response_time("Groq", model_name, start_time)
                logger.info(f"Finished streaming: processed {chunk_count} chunks in {completion_time:.2f}s")
                
                # Successful completion, break out of retry loop
                break
                
            except Exception as e:
                retry_count += 1
                log_api_response_time("Groq", model_name, start_time, status="error")
                
                # Enhanced error reporting
                error_type = type(e).__name__
                error_details = str(e)
                
                # Log detailed error information
                logger.error(f"API Error ({error_type}): {error_details}")
                
                # Create a detailed error panel for inspection
                error_panel = Panel(
                    f"Error Type: {error_type}\nDetails: {error_details}",
                    title="LLM API Error",
                    border_style="red",
                    expand=False
                )
                console.print(error_panel)
                
                # Check if we should retry
                if retry_count <= max_retries:
                    # Calculate backoff with exponential increase
                    wait_time = backoff_time * (2 ** (retry_count - 1))
                    logger.warning(f"Retrying in {wait_time:.1f}s (attempt {retry_count}/{max_retries})")
                    
                    # Yield a message to the user about the retry
                    yield f"\n[Experiencing a temporary issue. Retrying... ({retry_count}/{max_retries})]\n"
                    
                    # Wait before retry
                    time.sleep(wait_time)
                    
                    # Try alternative model if available and we're on the last retry
                    if retry_count == max_retries:
                        alt_model = "llama-3.1-8b-instant"
                        logger.info(f"Trying alternative model: {alt_model}")
                        yield f"\n[Switching to alternative model due to service issues...]\n"
                        
                        try:
                            log_api_request("Groq", alt_model)
                            alt_start_time = time.time()
                            
                            alt_completion = chat_client.chat.completions.create(
                                model=alt_model,
                                messages=final_messages,
                                temperature=0.2,
                                stream=True
                            )
                            
                            for chunk in alt_completion:
                                if chunk.choices[0].delta.content:
                                    yield chunk.choices[0].delta.content
                            
                            log_api_response_time("Groq", alt_model, alt_start_time)
                            break
                            
                        except Exception as alt_e:
                            log_api_response_time("Groq", alt_model, alt_start_time, "error")
                            logger.error(f"Alternative model also failed: {alt_e}")
                else:
                    # If all retries failed, yield a helpful error message
                    error_message = (
                        f"\n\nI apologize, but I'm experiencing technical difficulties connecting to my knowledge services. "
                        f"Error details: {error_type} - {error_details}\n\n"
                        f"Please try again in a few moments. If the problem persists, it may indicate an issue with the external API service."
                    )
                    yield error_message




            # trying with deepseek
            # Convert to OpenAI message format
            # openai_messages = convert_to_openai_messages(messages)
            # console.print(f"OpenAI converted messages w roles: {openai_messages}")
            # client = OpenAI(api_key=self.valves.deepseek_api_key, base_url="https://api.deepseek.com")
            # response = client.chat.completions.create(
            #     messages=openai_messages,
            #     stream=True
            # )
            # return response.choices[0].message.content

            # trying with mistral
            # Convert to OpenAI message format
            # openai_messages = convert_to_openai_messages(messages)
            # console.print(f"OpenAI converted messages w roles: {openai_messages}")
            # client = Mistral(api_key=self.valves.mistral_api_key)
            # chat_response = client.chat.complete(
            #     model="mistral-large-latest", temperature=0,
            #     messages = openai_messages
            # )
            # print(chat_response.choices[0].message.content)
            # return chat_response.choices[0].message.content





# TESTING PIPE
# Note: there are other test functions in the tests/test_router.py file
# This is a simple test function to test the pipe with a single user message

def test_pipe():
    """
    Basic test function for the Beacon pipeline.
    """
    # Initialize the Pipe with local testing enabled
    pipe = Pipe(local_testing=True)

    # Mock user data
    __user__ = {
        "id": "test123",
        "email": "test@example.com", 
        "name": "Test User",
        "role": "tester"
    }
    
    # Mock request (not used in local testing)
    __request__ = None

    async def run_test():
        # Test conversation
        test_messages = {
            # Full conversation history with shared context about human rights in Russia
            # "messages": [
            #     {"role": "user", "content": "I'm researching human rights situations in different countries. Can you tell me about Russia?"},
            #     {"role": "assistant", "content": "I'd be happy to help with your research on human rights in Russia. Russia has a complex human rights record with various concerns raised by international organizations. Would you like me to focus on any specific aspects of human rights in Russia, such as freedom of expression, political rights, or treatment of minorities?"},
            #     {"role": "user", "content": "Yes, please tell me about freedom of expression and political rights in Russia."},
            #     {"role": "assistant", "content": "Regarding freedom of expression and political rights in Russia:\n\n**Freedom of Expression:**\n- Media outlets face significant restrictions, with many independent media organizations labeled as \"foreign agents\"\n- Internet censorship has increased, with authorities blocking websites and platforms\n- Journalists reporting on sensitive topics have faced harassment, prosecution, and in some cases violence\n\n**Political Rights:**\n- Opposition figures have reported obstacles to participating in elections\n- Public demonstrations require advance approval, and unauthorized protests often result in detentions\n- Several opposition leaders and activists have faced legal challenges, including imprisonment\n\nWould you like more specific information about any of these points or information about other human rights aspects in Russia?"},
            #     {"role": "user", "content": "How does this compare to the human rights situation in the USA?"}
            # ]
            "messages": [
            #     # {"role": "user", "content": "What is the latest news on the situation in Ukraine? How is the conflict classified in Ukraine?"}
                # {"role": "user", "content": "Who is the current prime minister of France? Is there a conflict in Ukraine according to RULAC?"}
            #     #  {"role": "user", "content": "I'm curious about Ukraine. How is the conflict classified in Ukraine? What has Ukraine done to protect human rights in terms of the Rome Statute?"}
                {"role": "user", "content": "Has Ukraine signed the Rome Statute according to HRW?"}
                # {"role": "user", "content": "is russia or usa more dangerous for human rights? why?"}
                # {"role": "user", "content": "what conflicts are taking place in USA and Russia?"}
            #     # {"role": "user", "content": "what is the diff between france and russia's human rights records in LGBT rights?  what are the similiarities?"}
                # {"role": "user", "content": "what is international human rights law?"}
                # {"role": "user", "content": "what is RULAC and who made it? How do they classify the conflict in Ukraine? What about the conflict in Syria?"}
            #     # {"role": "user", "content": "what is the difference between international humanitarian law and international human rights law? What role does the UN and ECJ play in this? How are war crimes prosecuted? Does the ICC essentially prosecute IHL violations?"},
            #     # {"role": "user", "content": "how does RULAC classify the conflict in france?"},
                # {"role": "user", "content": "what are your capabilities? Is there a conflict in Ukraine according to RULAC?"},
                # {"role": "user", "content": "who is the voguer vinii revlon? what is the house of revlon?"},
                # {"role": "user", "content": "what is your name? What tasks can you perform? "},
                # {"role": "user", "content": "How is the Ukraine conflict classified? and who is the current prime minister of France?"},
            #     # {"role": "user", "content": "what is the latest news on the situation in Ukraine? Is there a ceasefire?"}
            #     # {"role": "user", "content": "how is the conflict in ukraine classified? what is the applicable IHL law? and can you give me a timeline of the conflict's key events?"}
                 
            ]
        }

        # Track execution time
        start_time = time.time()
        
        # Run the pipeline
        response = await pipe.pipe(
            test_messages,
            __user__,
            __request__,
            local_testing=True
        )

        # Calculate execution time
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Display execution time
        console.print(f"\n[green]‚úì Test completed in {execution_time:.2f} seconds[/]")

        # Collect and display response
        final_answer = ""
        print("\nStreaming response:")
        for token in response:
            # print(token, end="")
            final_answer += token

        # Display final answer in formatted panel
        panel = Panel(
            Markdown(final_answer),
            style="white on black",
            border_style="purple3",
            title="Test Response",
            title_align="left"
        )
        console.print("\n")
        console.print(panel)

    # Run the test
    asyncio.run(run_test())



def test_title_generation():
    """Test the title generation functionality of the pipeline"""
    # Initialize the Pipe with local testing enabled
    pipe = Pipe(local_testing=True)

    # Mock user data
    __user__ = {
        "id": "test123",
        "email": "test@example.com", 
        "name": "Test User",
        "role": "tester"
    }

    # Sample body with query for title generation
    test_body = {
        "model": "beacon_agent_v1_mar_2025",
        "messages": [{
            "role": "user",
            "content": """Here is the query:
What is the human rights situation in Somalia?

Create a concise, 3-5 word phrase as a title for the previous query. Avoid quotation marks or special formatting. RESPOND ONLY WITH THE TITLE TEXT.

Examples of titles:
Ukraine Conflict Classification
Uganda Press Freedom
Racial Justice in USA
EU Judicial Reform
LGBTQ+ Rights in China"""
        }],
        "stream": False,
        "max_completion_tokens": 1000
    }

    async def run_test():
        print("\n[bold yellow]Testing Title Generation[/]")
        result = await pipe.pipe(
            test_body, 
            __user__, 
            None, 
            local_testing=True,
            __task__="title_generation"
        )
        print(f"\nGenerated Title: {result}")

    # Run the test
    asyncio.run(run_test())

if __name__ == "__main__":
    # test_pipe()
    print("\n" + "="*50 + "\n")
    test_title_generation()


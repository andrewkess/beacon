from typing import List, Dict, Any, Union, Optional, TypedDict, Literal, Tuple
from langchain_core.tools import tool, BaseTool
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
import logging
import os
import coloredlogs
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse, quote, urlunparse
import pprint
import json
from datetime import datetime
from fake_useragent import UserAgent
import random
import time
from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage
import glob
from pydantic import BaseModel, Field

# Import standardized functions from RULAC_tools
from tools.RULAC_tools import (
    Citation, 
    RULAC_TOOL_RESULT,
    create_standard_citation, 
    format_standard_tool_result,
    standardized_tool_test,
    display_formatted_results
)

# Global configuration values
tool_specific_values = {
    "BRAVE_SEARCH_API_BASE_URL": "https://api.search.brave.com/res/v1/web/search",
    "BRAVE_SEARCH_NEWS_API_BASE_URL": "https://api.search.brave.com/res/v1/news/search",
    "BRAVE_SEARCH_API_KEY": "BSALK92N6p6OSBKHULa_G0gBpSuTtT6",
    "PAGE_CONTENT_WORDS_LIMIT": 4000,
}

# Initialize fake user agent
ua = UserAgent()

# Common headers that we'll rotate through
COMMON_HEADERS = [
    {
        "Accept": "application/json",
        "Accept-Encoding": "gzip",
        "X-Subscription-Token": tool_specific_values["BRAVE_SEARCH_API_KEY"]
    }
]

def get_request_headers(custom_headers: Optional[Dict[str, str]] = None) -> Dict[str, str]:
    """
    Generate headers for HTTP requests, optionally merging with custom headers.
    
    Args:
        custom_headers: Optional dictionary of custom headers to merge with defaults
        
    Returns:
        Dictionary of headers to use for the request
    """
    # Get a random user agent
    user_agent = ua.random
    logger.debug(f"Generated User-Agent: {user_agent}")
    
    # Get a random set of common headers
    base_headers = random.choice(COMMON_HEADERS).copy()
    
    # Add the random user agent
    base_headers["User-Agent"] = user_agent
    
    # Merge with custom headers if provided
    if custom_headers:
        base_headers.update(custom_headers)
    
    return base_headers

# Initialize Rich Console for formatted output
console = Console()

# Configure logging
logger = logging.getLogger("Beacon.HRW")
coloredlogs.install(
    logger=logger,
    level="DEBUG",
    isatty=True,
    fmt="%(asctime)s [%(levelname)s] %(message)s",
)

# Define log styles for rich console output
LOG_STYLES = {
    "info": {"style": "black on yellow", "border_style": "yellow"},
    "success": {"style": "black on green", "border_style": "green"},
    "error": {"style": "white on red", "border_style": "red"},
    "title": {"style": "bold white on green", "border_style": "green"}
}

def log(message, level="info", as_panel=True):
    """
    Log a message with consistent styling based on level.
    
    Args:
        message: The message to log
        level: The log level (info, success, error, title)
        as_panel: Whether to format as a panel or simple text
    """
    style = LOG_STYLES.get(level, LOG_STYLES["info"])
    
    if as_panel:
        console.print(Panel(message, **style))
    else:
        console.print(message, style=style["style"])

# Common function for article content scraping
async def generic_article_scraper(
    urls: List[str], 
    source_name: str,
    find_article_content: callable,
    clean_content: callable
) -> List[str]:
    """
    Generic function for scraping article content that can be configured for different news sources.
    
    Args:
        urls: List of URLs to scrape
        source_name: Name of the news source for logging
        find_article_content: Function that takes soup and returns the article content element
        clean_content: Function that takes article_content and returns cleaned text
        
    Returns:
        List of article text contents
    """
    # Create temp directory if it doesn't exist
    temp_dir = "temp_test_data"
    os.makedirs(temp_dir, exist_ok=True)
    
    # Initialize results
    article_texts = []
    
    # Process each URL
    for url in urls:
        try:
            logger.debug(f"Scraping {source_name} URL: {url}")
                
            # Generate headers for this request
            headers = get_request_headers()
                
            # Fetch the page content
            response = requests.get(url, headers=headers, timeout=120)
            response.raise_for_status()
            
            # Parse the HTML
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Find the main article content using the provided function
            article_content = find_article_content(soup)
            
            # Process the content if article was found
            if article_content:
                # Use the provided cleaning function
                text = clean_content(article_content)
                
                # Log text statistics
                logger.debug(f"Text length before truncation: {len(text.split())} words")
                
                # Limit to specified number of words
                words = text.split()
                if len(words) > tool_specific_values["PAGE_CONTENT_WORDS_LIMIT"]:
                    text = " ".join(words[:tool_specific_values["PAGE_CONTENT_WORDS_LIMIT"]])
                    logger.debug(f"Truncated content to {tool_specific_values['PAGE_CONTENT_WORDS_LIMIT']} words")
            else:
                text = ""
                logger.warning(f"Could not find main content container for {source_name}")
            
            # Add the text content to the results
            article_texts.append(text)
            
            logger.debug(f"Successfully scraped: {url}")
                
        except Exception as e:
            logger.error(f"Error scraping {url}: {str(e)}")
            logger.error(f"Error type: {type(e).__name__}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            # Add empty content for failed scrapes
            article_texts.append("")
    
    return article_texts

# BBC News content finder
def find_bbc_content(soup):
    return soup.find('article')

# BBC News content cleaner
def clean_bbc_content(article_content):
    # Remove all script and style elements
    for element in article_content.find_all(['script', 'style']):
        element.decompose()
    
    # Remove all navigation elements
    for nav in article_content.find_all('nav'):
        nav.decompose()
    
    # Remove all button elements (often used for sharing, etc)
    for button in article_content.find_all('button'):
        button.decompose()
    
    # Remove all ul elements (typically contains related links)
    for element in article_content.find_all('ul'):
        element.decompose()
    
    # Remove links blocks and other components
    for element in article_content.find_all(attrs={
        "data-component": [
            "topic-list",
            "tag-list",
            "share-tools",
            "recommendations",
            "related-content",
            "links-block"
        ]
    }):
        element.decompose()
    
    # Remove specific BBC elements that contain related content
    for element in article_content.find_all(class_=['topic-list', 'article__topics', 'article-share', 'article-footer']):
        element.decompose()
    
    # Process paragraphs and headers to preserve structure
    paragraphs = []
    
    # Find all text-containing elements, focusing on actual article content
    for element in article_content.find_all(['p', 'h2', 'h3', 'h4', 'h5', 'h6']):
        # Skip elements that are part of navigation, sharing, or related content
        if element.find_parent(attrs={"data-component": ["links-block", "topic-list", "tag-list", "share-tools", 
                                                       "recommendations", "related-content"]}):
            continue
            
        # Process all text nodes and links within the element
        text_parts = []
        for content in element.contents:
            if isinstance(content, str):  # Text node
                text_parts.append(content.strip())
            elif content.name == 'a':  # Link
                # Skip links that are likely to be related content
                if not content.find_parent(attrs={"data-component": ["links-block", "topic-list", "tag-list"]}):
                    text_parts.append(content.get_text(strip=True))
            elif content.name in ['em', 'i', 'strong', 'b', 'span']:  # Inline formatting
                text_parts.append(content.get_text(strip=True))
        
        # Join all parts and normalize spaces
        text = ' '.join(text_parts)
        text = re.sub(r'\s+', ' ', text).strip()
        
        if text:  # Only add non-empty paragraphs
            paragraphs.append(text)
    
    # Join paragraphs with double newlines to preserve structure
    text = '\n\n'.join(paragraphs)
    
    # Clean up the text
    text = re.sub(r'[ \t]+', ' ', text)  # Clean up spaces and tabs, preserve newlines
    text = re.sub(r'\s+([,\.])', r'\1', text)  # Remove spaces before punctuation
    text = text.strip()
    
    return text

# Al Jazeera content finder
def find_aljazeera_content(soup):
    return soup.find('div', class_='wysiwyg wysiwyg--all-content')

# Al Jazeera content cleaner
def clean_aljazeera_content(article_content):
    # First, remove all unwanted elements
    elements_to_remove = [
        # Remove recommended stories
        {'class_': 'more-on'},
        # Remove all ad containers
        {'class_': 'container--ads'},
        # Remove newsletter signup
        {'class_': 'article-newsletter-slot'},
        # Remove screen reader text
        {'class_': 'screen-reader-text'}
    ]
    
    for criteria in elements_to_remove:
        for element in article_content.find_all(class_=criteria['class_']):
            element.decompose()
    
    # Process content in order to maintain structure
    content_elements = []
    
    # Helper function to process text from an element
    def process_element_text(element):
        text_parts = []
        for content in element.contents:
            if isinstance(content, str):
                text_parts.append(content.strip())
            elif content.name == 'a':
                text_parts.append(content.get_text(strip=True))
            elif content.name in ['em', 'i', 'strong', 'b', 'span', 'p']:
                text_parts.append(content.get_text(strip=True))
        
        # Join and clean the text
        text = ' '.join(text_parts)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    # First, handle headings and paragraphs at the top level
    for element in article_content.children:
        # Skip if not a tag
        if not hasattr(element, 'name'):
            continue
        
        # Process based on element type
        if element.name in ['p', 'h2', 'h3']:
            text = process_element_text(element)
            
            # Add heading marker for h2/h3
            if element.name == 'h2':
                text = f"\n## {text}\n"
            elif element.name == 'h3':
                text = f"\n### {text}\n"
                
            if text:  # Only add non-empty elements
                content_elements.append(text)
        
        # Handle lists at the top level
        elif element.name == 'ul':
            # Process each list item
            for li in element.find_all('li', recursive=False):
                list_text = "- " + process_element_text(li)
                if list_text:  # Only add non-empty elements
                    content_elements.append(list_text)
    
    # Now also find all lists in the article (for nested lists)
    for ul in article_content.find_all('ul'):
        # Check if we already processed this list at the top level
        if ul.parent == article_content:
            continue  # Skip already processed lists
            
        # Process each list item in this nested list
        for li in ul.find_all('li', recursive=False):
            list_text = "- " + process_element_text(li)
            if list_text:  # Only add non-empty elements
                content_elements.append(list_text)
    
    # Join all elements with appropriate spacing
    text = '\n'.join(content_elements)
    
    # Clean up the text
    text = re.sub(r'\n{3,}', '\n\n', text)  # Replace multiple newlines with double newlines
    text = re.sub(r'[ \t]+', ' ', text)  # Clean up spaces and tabs
    text = re.sub(r'\s+([,\.])', r'\1', text)  # Remove spaces before punctuation
    text = text.strip()
    
    return text

# AP News content finder
def find_ap_content(soup):
    # Look for the main article content - AP News typically uses RichTextStoryBody
    article_content = soup.find('div', class_='RichTextStoryBody RichTextBody')
    
    # If RichTextStoryBody isn't found, try the Article class as fallback
    if not article_content:
        article_content = soup.find('div', class_='Article')
    
    # Last resort - try to find any main content container
    if not article_content:
        article_content = soup.find('main') or soup.find('article')
        
    return article_content

# AP News content cleaner
def clean_ap_content(article_content):
    # For the fallback case where we found main/article
    if article_content.name in ['main', 'article'] and not article_content.find(class_='RichTextStoryBody'):
        paragraphs = []
        for element in article_content.find_all(['p', 'h2', 'h3', 'h4']):
            if element.name.startswith('h'):
                paragraphs.append(f"\n## {element.get_text(strip=True)}\n")
            else:
                paragraphs.append(element.get_text(strip=True))
        
        text = '\n\n'.join(paragraphs)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    # Regular AP article
    # Remove unwanted elements
    elements_to_remove = [
        # Remove ad containers
        {'class_': 'ad-placeholder'},
        {'class_': 'SovrnAd'},
        {'class_': 'Advertisement'},
        # Remove related content
        {'class_': 'Related'},
        {'class_': 'PageListEnhancementGeneric'},
        {'class_': 'HTMLModuleEnhancement'},
        # Remove social media sharing
        {'class_': 'social-share'},
        # Remove newsletter signup
        {'class_': 'newsletter-subscribe'},
        # Remove media components that are not part of the main article
        {'class_': 'Media-caption'}
    ]
    
    for criteria in elements_to_remove:
        for attribute, value in criteria.items():
            for element in article_content.find_all(class_=value):
                element.decompose()
    
    # Process article content to extract text
    paragraphs = []
    
    # Process headings first
    for heading in article_content.find_all(['h1', 'h2', 'h3', 'h4']):
        heading_text = heading.get_text(strip=True)
        if heading_text:
            paragraphs.append(f"\n## {heading_text}\n")
    
    # Then process paragraphs
    for para in article_content.find_all('p'):
        # Skip paragraphs within unwanted elements that weren't caught earlier
        if para.find_parent(class_=['SovrnAd', 'Advertisement', 'Related', 'social-share', 'newsletter-subscribe']):
            continue
        
        # Process text within the paragraph
        text_parts = []
        for content in para.contents:
            if isinstance(content, str):  # Text node
                text_parts.append(content.strip())
            elif content.name == 'a':  # Link
                text_parts.append(content.get_text(strip=True))
            elif content.name in ['em', 'i', 'strong', 'b', 'span']:  # Inline formatting
                text_parts.append(content.get_text(strip=True))
        
        # Join and clean text
        text = ' '.join(text_parts)
        text = re.sub(r'\s+', ' ', text).strip()
        
        if text:  # Only add non-empty paragraphs
            paragraphs.append(text)
    
    # Join paragraphs with double newlines to preserve structure
    text = '\n\n'.join(paragraphs)
    
    # Clean up the text
    text = re.sub(r'[ \t]+', ' ', text)  # Clean up spaces and tabs
    text = re.sub(r'\s+([,\.])', r'\1', text)  # Remove spaces before punctuation
    text = re.sub(r'\n{3,}', '\n\n', text)  # Replace multiple newlines with double newlines
    text = text.strip()
    
    return text

# Replace the original scraping functions with versions that use the generic scraper
async def async_scrape_BBC_news(urls: List[str]) -> List[str]:
    """
    Asynchronously scrape multiple BBC URLs using the generic scraper.
    
    Args:
        urls: List of URLs to scrape
        
    Returns:
        List of article text content
    """
    return await generic_article_scraper(
        urls=urls,
        source_name="BBC",
        find_article_content=find_bbc_content,
        clean_content=clean_bbc_content
    )

async def async_scrape_AlJazeera_news(urls: List[str]) -> List[str]:
    """
    Asynchronously scrape multiple Al Jazeera URLs using the generic scraper.
    
    Args:
        urls: List of URLs to scrape
        
    Returns:
        List of article text content
    """
    return await generic_article_scraper(
        urls=urls,
        source_name="Al Jazeera",
        find_article_content=find_aljazeera_content,
        clean_content=clean_aljazeera_content
    )

async def async_scrape_Reuters_news(urls: List[str]) -> List[str]:
    """
    Placeholder function that returns empty strings since Reuters scraping is skipped.
    This function exists only to satisfy the function reference in get_latest_news_from_Reuters.
    
    Args:
        urls: List of URLs that won't be scraped
        
    Returns:
        List of empty strings matching the length of the input URLs list
    """
    return [""] * len(urls)

async def async_scrape_AP_news(urls: List[str]) -> List[str]:
    """
    Asynchronously scrape multiple Associated Press (AP) URLs using the generic scraper.
    
    Args:
        urls: List of URLs to scrape
        
    Returns:
        List of article text content
    """
    return await generic_article_scraper(
        urls=urls,
        source_name="AP News",
        find_article_content=find_ap_content,
        clean_content=clean_ap_content
    )

async def get_search_results(query: str, number_of_results: int = 1, search_type: Literal["web", "news"] = "web") -> List[Dict[str, Any]]:
    """
    Get search results from the Brave Search API.
    
    Args:
        query: The search query string
        number_of_results: Maximum number of results to return
        search_type: Type of search to perform ("web" or "news")
        
    Returns:
        List of search result dictionaries
    """
    try:
        log(f"Connecting to Brave Search API for {search_type} search")
        
        # Set up query parameters
        params = {
            "q": query,
            "count": number_of_results,
            "search_lang": "en",
            "ui_lang": "en-US",
            "safesearch": "off",
            "text_decorations": "1",
            "spellcheck": "1",
        }
        
        # Set result filter based on search type
        if search_type == "web":
            params["result_filter"] = "web"
        elif search_type == "news":
            params["result_filter"] = "news"
        
        # Select appropriate API endpoint based on search type
        api_url = (tool_specific_values["BRAVE_SEARCH_NEWS_API_BASE_URL"] 
                  if search_type == "news" 
                  else tool_specific_values["BRAVE_SEARCH_API_BASE_URL"])
        
        logger.debug(f"Using API endpoint: {api_url}")
        logger.debug(f"Search parameters: {json.dumps(params, indent=2)}")
        
        # Generate headers for this request
        headers = get_request_headers()
        logger.debug(f"Request headers: {json.dumps(headers, indent=2)}")
        
        # Send the request to the Brave Search API
        resp = requests.get(
            api_url,
            params=params,
            headers=headers,
            timeout=120
        )
        resp.raise_for_status()
        data = resp.json()
        
        # Log the full response for debugging
        # log(f"Full API Response for {search_type} search:", "info")
        # log(json.dumps(data, indent=2), "info", False)

        # Extract results from Brave Search response
        results = []
        
        # Process results based on search type
        if search_type == "news":
            if "results" in data:
                logger.debug(f"Found {len(data['results'])} news results")
                for result in data["results"]:
                    results.append({
                        "title": result.get("title", ""),
                        "link": result.get("url", ""),
                        "snippet": result.get("description", ""),
                        "category": "news",
                        "publication_date": result.get("page_age", ""),  # News API uses page_age
                        "source": result.get("meta_url", {}).get("hostname", ""),
                        "thumbnail": result.get("thumbnail", {}).get("src", "")
                    })
        else:  # web search
            if "web" in data and "results" in data["web"]:
                logger.debug(f"Found {len(data['web']['results'])} web results")
                for result in data["web"]["results"]:
                    results.append({
                        "title": result.get("title", ""),
                        "link": result.get("url", ""),
                        "snippet": result.get("description", ""),
                        "category": "web",
                        "publication_date": result.get("page_age", "")
                    })
        
        logger.debug(f"Total results after processing: {len(results)}")
        log(f"Found {len(results)} search results", "success")
        return results
        
    except requests.exceptions.RequestException as e:
        error_msg = f"Search engine error for {search_type} search: {str(e)}"
        log(error_msg, "error")
        logger.error(f"Full error details: {str(e)}")
        logger.error(f"Error type: {type(e).__name__}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return []

class ArticleSummary(BaseModel):
    """Model for a single article summary"""
    title: str = Field(..., description="The title of the article")
    date: str = Field(..., description="The publication date of the article")
    content: str = Field(..., description="The content/summary of the article")

class NewsSourceResponse(BaseModel):
    """Model for the response from a news source"""
    source_name: str = Field(..., description="The name of the news source (e.g., BBC, Al Jazeera)")
    articles: List[ArticleSummary] = Field(..., description="List of article summaries")
    total_articles: int = Field(..., description="Total number of articles found")

async def get_latest_news_from_source(
    search_query: str, 
    source_name: str, 
    source_url: str,
    web_results_count: int,
    news_results_count: int,
    scrape_function: callable,
    skip_scraping_and_use_AI_summary: bool = False
) -> NewsSourceResponse:
    """
    Helper function that retrieves the latest news articles from a specified news source.
    
    Args:
        search_query: The search query to find relevant news articles
        source_name: The name of the news source (e.g., "BBC", "Al Jazeera")
        source_url: The base URL for the news source (e.g., "www.bbc.com/news")
        web_results_count: Number of web search results to fetch (0 to skip web search)
        news_results_count: Number of news search results to fetch (0 to skip news search)
        scrape_function: The specific scraping function to use for this source
        skip_scraping_and_use_AI_summary: If True, uses the snippet/description from API instead of scraping
        
    Returns:
        A standardized NewsSourceResponse
    """
    tool_name = f"get_latest_news_from_{source_name.replace(' ', '')}"
    
    # Print start marker for tool execution
    console.print("\n[bold white]" + "="*50 + "\n" + 
                 f"STARTING TOOL: {tool_name}\n" + 
                 "="*50 + "[/bold white]\n")
    
    # Set up tool metadata for result
    research_task = f"Retrieve {source_name} News articles for: {search_query}"
    UI_facing_research_task = f"Researching {source_name} News articles about: {search_query}"
    tool_params = {
        "search_query": search_query,
        "research_task": research_task,
        "UI_facing_research_task": UI_facing_research_task
    }
    
    try:
        # Construct the search query with site filter
        initial_search_query = search_query
        search_query = f'site:{source_url} {search_query}'
        log(f"Searching {source_name} News for articles about: {initial_search_query}")
        log(f"Search query: {search_query}")
        
        # Initialize results containers
        web_search_results = []
        news_search_results = []
        
        # Get web search results if count > 0
        if web_results_count > 0:
            web_search_results = await get_search_results(
                query=search_query,
                number_of_results=web_results_count,
                search_type="web"
            )
            log(f"Found {len(web_search_results)} web search results", "info")
        else:
            log("Skipping web search as web_results_count is 0", "info")
        
        # Get news search results if count > 0
        if news_results_count > 0:
            news_search_results = await get_search_results(
                query=search_query,
                number_of_results=news_results_count,
                search_type="news"
            )
            log(f"Found {len(news_search_results)} news search results", "info")
        else:
            log("Skipping news search as news_results_count is 0", "info")
        
        # Remove duplicates based on URL before combining results
        seen_urls = set()
        deduplicated_results = []
        
        # Helper function to add unique results
        def add_unique_results(results):
            for result in results:
                url = result.get('link')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    deduplicated_results.append(result)
        
        # Add web results first (they tend to be more comprehensive)
        add_unique_results(web_search_results)
        # Then add news results
        add_unique_results(news_search_results)
        
        # Sort results by publication date in ascending order (oldest first)
        deduplicated_results.sort(key=lambda x: x.get("publication_date", ""))
        
        # Use deduplicated results instead of raw combination
        search_results = deduplicated_results
        
        # Pretty print the results for debugging
        log("Search Results:", "info")
        log(f"Found {len(web_search_results)} web results and {len(news_search_results)} news results", "info")
        for i, result in enumerate(search_results, 1):
            log(f"\nResult {i}:", "info", False)
            log(f"Title: {result.get('title', 'No title')}", "info", False)
            log(f"Link: {result.get('link', 'No link')}", "info", False)
            log(f"Snippet: {result.get('snippet', 'No snippet')}", "info", False)
            log(f"Category: {result.get('category', 'No category')}", "info", False)
            log(f"Source: {result.get('source', 'No source')}", "info", False)
            log(f"Publication Date: {result.get('publication_date', 'No date')}", "info", False)
            log("-" * 50, "info", False)
        
        # Initialize containers for results
        formatted_results = []
        citations = []
        
        if search_results:
            log(f"Processing {len(search_results)} search results")
            
            # Get URLs from results
            urls = [r["link"] for r in search_results]
            
            if skip_scraping_and_use_AI_summary:
                log(f"Skipping scraping and using snippets from search results")
                
                # Use snippets from search results instead of scraping
                for i, result in enumerate(search_results):
                    url = result.get("link", "")
                    title = result.get("title", f"{source_name} News article about {initial_search_query}")
                    publication_date = result.get("publication_date", "")
                    
                    # Use snippet as content
                    content = result.get("snippet", "")
                    
                    if not content:
                        continue
                    
                    # Format article with XML structure
                    logger.debug(f"Formatting article with title: {title}")
                    formatted_article = f"""<article>\n<title>{title}</title>\n<publication_date>{publication_date}</publication_date>\n<content>{content}</content>\n</article>"""
                    logger.debug("XML Structure created:")
                    logger.debug(formatted_article[:500] + "..." if len(formatted_article) > 500 else formatted_article)
                    
                    formatted_results.append(formatted_article)
                    logger.debug(f"Total articles formatted so far: {len(formatted_results)}")
                    
                    # Create citation using standard format
                    citation = create_standard_citation(
                        title=title,
                        url=url,
                        formatted_content=content
                    )
                    citations.append(citation)
            else:
                try:
                    log(f"Scraping content from {len(urls)} pages")
                    article_texts = await scrape_function(urls)
                    
                    # Process and format results
                    for url, title, publication_date, article_text in zip(urls, [r["title"] for r in search_results], 
                                                                         [r["publication_date"] for r in search_results], 
                                                                         article_texts):
                        if not article_text:
                            continue
                        
                        # Format article with XML structure
                        logger.debug(f"Formatting article with title: {title}")
                        formatted_article = f"""<article>\n<title>{title}</title>\n<publication_date>{publication_date}</publication_date>\n<content>{article_text}</content>\n</article>"""
                        logger.debug("XML Structure created:")
                        logger.debug(formatted_article[:500] + "..." if len(formatted_article) > 500 else formatted_article)
                        
                        formatted_results.append(formatted_article)
                        logger.debug(f"Total articles formatted so far: {len(formatted_results)}")
                        
                        # Create citation using standard format
                        citation = create_standard_citation(
                            title=title if title else f"{source_name} News article about {initial_search_query}",
                            url=url,
                            formatted_content=article_text
                        )
                        citations.append(citation)
                    
                except Exception as e:
                    log(f"Error during page scraping: {str(e)}", "error")
                    logger.error(f"Full error details: {str(e)}")
                    logger.error(f"Error type: {type(e).__name__}")
                    import traceback
                    logger.error(f"Traceback: {traceback.format_exc()}")
        else:
            log("No search results found", "error")
        
        log(f"Search complete. Found {len(formatted_results)} relevant articles", "success")
        
        # Prepare the final content for prompt, including source-specific tags
        tag_name = f"{source_name.replace(' ', '_')}_News"
        logger.debug(f"Creating final content with {tag_name} tags")
        final_content = f"<{tag_name}>\nLatest {source_name} News Articles about: {initial_search_query}" + "\n\n".join(formatted_results) + f"\n</{tag_name}>"
        logger.debug("Final content structure (first 500 chars):")
        logger.debug(final_content[:500] + "..." if len(final_content) > 500 else final_content)

        # If we have articles, generate an AI summary using Groq
        article_summaries = []
        if formatted_results:
            # Display formatted results in Console 
            console.print(Panel(Markdown(final_content), title=f"{source_name.upper()} NEWS ARTICLES", border_style="green"))

            log("Generating AI summary of articles using Groq LLM...", "info")
            groq_start_time = time.time()
            
            # Groq API key
            groq_api_key = "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"
            
            # Initialize ChatGroq with appropriate model
            chat = ChatGroq(
                groq_api_key=groq_api_key,
                # model="llama-3.3-70b-versatile",
                            # model="deepseek-r1-distill-qwen-32b",
            # model="llama3-70b-8192", #only 8k token window
            model="llama-3.1-8b-instant", #only 8k token window
                temperature=0,
            )
            
            # Prepare system prompt and messages
            system_prompt = f"""You are an expert news synthesizer. 

            Here are the {source_name} news articles to synthesize:

            {final_content}

            ---
            For each article, provide a structured summary with:
            1. Title: The article's title
            2. Date: The publication date
            3. Content: A concise summary of the main points, focusing on the most important information and maintaining objectivity.
            
            Return your response as a JSON array of objects, where each object has:
            {{
                "title": "string",
                "date": "string",
                "content": "string"
            }}

            Make sure to return a valid JSON only.
            """
            
            # Create messages for ChatGroq
            messages = [
                SystemMessage(content=system_prompt),
            ]
            
            # Make the LLM call using ChatGroq
            try:
                response = chat.invoke(messages)
                raw_content = response.content
                log(f"Raw response content: {raw_content[:500]}...", "info")  # Changed error to info
                
                # Clean the response before parsing JSON
                # Remove markdown code blocks, whitespace, and other formatting
                cleaned_content = raw_content
                
                # Remove markdown code blocks if present
                if "```json" in cleaned_content:
                    cleaned_content = cleaned_content.split("```json")[1]
                    if "```" in cleaned_content:
                        cleaned_content = cleaned_content.split("```")[0]
                
                # Remove any text before the first '[' character
                if '[' in cleaned_content:
                    cleaned_content = cleaned_content[cleaned_content.find('['):]
                
                # Remove any text after the last ']' character
                if ']' in cleaned_content:
                    cleaned_content = cleaned_content[:cleaned_content.rfind(']')+1]
                
                cleaned_content = cleaned_content.strip()
                
                log(f"Cleaned JSON: {cleaned_content[:100]}...", "info")
                
                # Parse the JSON response
                try:
                    article_summaries = json.loads(cleaned_content)
                    # Validate each article summary against our model
                    validated_summaries = []
                    for summary in article_summaries:
                        validated_summary = ArticleSummary(**summary)
                        validated_summaries.append(validated_summary)
                    article_summaries = validated_summaries
                    
                    groq_elapsed = time.time() - groq_start_time
                    log(f"Generated AI summary in {groq_elapsed:.2f} seconds", "success")
                    
                    # Display the AI summary
                    log(f"\nAI SUMMARY OF {source_name.upper()} ARTICLES:", "title")
                    console.print(Panel(Markdown(raw_content), title=f"Summary of {source_name} News Articles", border_style="green"))
                    
                except json.JSONDecodeError as e:
                    log(f"Error parsing AI summary JSON: {str(e)}", "error")
                    log(f"Attempted to parse: {cleaned_content}", "error")
                    article_summaries = []
                except Exception as e:
                    log(f"Error validating article summaries: {str(e)}", "error")
                    article_summaries = []
                
            except Exception as e:
                log(f"Error generating AI summary: {str(e)}", "error")
                log(f"Error type: {type(e).__name__}", "error")
                import traceback
                log(f"Traceback: {traceback.format_exc()}", "error")
                article_summaries = []
        
        # Return no results if none found
        if not article_summaries:
            # Print end marker for tool execution
            console.print("\n[bold white]" + "="*50 + "\n" + 
                         f"ENDING TOOL: {tool_name} (no results)\n" + 
                         "="*50 + "[/bold white]\n")
                         
            # Return empty response
            return NewsSourceResponse(
                source_name=source_name,
                articles=[],
                total_articles=0
            )
        
        # Print end marker for tool execution
        console.print("\n[bold white]" + "="*50 + "\n" + 
                     f"ENDING TOOL: {tool_name} (success)\n" + 
                     "="*50 + "[/bold white]\n")
        
        # Return structured response
        return NewsSourceResponse(
            source_name=source_name,
            articles=article_summaries,
            total_articles=len(article_summaries)
        )
    
    except Exception as e:
        error_message = f"Error retrieving {source_name} News articles: {str(e)}"
        log(error_message, "error")
        
        # Print end marker for tool execution
        console.print("\n[bold white]" + "="*50 + "\n" + 
                     f"ENDING TOOL: {tool_name} (with error)\n" + 
                     "="*50 + "[/bold white]\n")
        
        # Return error response
        return NewsSourceResponse(
            source_name=source_name,
            articles=[],
            total_articles=0
        )

@tool
async def get_latest_news_from_BBC(search_query: str) -> RULAC_TOOL_RESULT:
    """
    Retrieves the latest news articles from BBC News based on the provided search query.
    
    :param search_query: The search query to find relevant BBC news articles
    :return: A dictionary with "content" containing the article content and "citations" list
    """
    result = await get_latest_news_from_source(
        search_query=search_query,
        source_name="BBC",
        source_url="www.bbc.com/news/articles",
        web_results_count=2,
        news_results_count=1,
        scrape_function=async_scrape_BBC_news
    )
    
    # Convert NewsSourceResponse to RULAC_TOOL_RESULT format
    if result.total_articles == 0:
        return format_standard_tool_result(
            content="No relevant BBC News articles found for this query.",
            citations=[],
            tool_name="get_latest_news_from_BBC",
            tool_params={"search_query": search_query},
            beacon_tool_source="BBC"
        )
    
    # Format articles into content string
    content = f"# BBC News Articles about: {search_query}\n\n"
    for article in result.articles:
        content += f"## {article.title}\n"
        content += f"*Published: {article.date}*\n\n"
        content += f"{article.content}\n\n"
    
    # Create citations from articles
    citations = []
    for article in result.articles:
        citation = create_standard_citation(
            title=article.title,
            url="",  # URL not available in new format
            formatted_content=article.content
        )
        citations.append(citation)
    
    logger.debug(f"BBC News content created: {len(content)} chars")
    
    return format_standard_tool_result(
        content=content,
        citations=citations,
        tool_name="get_latest_news_from_BBC",
        tool_params={"search_query": search_query},
        beacon_tool_source="BBC"
    )

@tool
async def get_latest_news_from_AlJazeera(search_query: str) -> RULAC_TOOL_RESULT:
    """
    Retrieves the latest news articles from Al Jazeera News based on the provided search query.
    
    :param search_query: The search query to find relevant Al Jazeera news articles
    :return: A dictionary with "content" containing the article content and "citations" list
    """
    result = await get_latest_news_from_source(
        search_query=search_query,
        source_name="AlJazeera",
        source_url="www.aljazeera.com/news",
        web_results_count=1,
        news_results_count=2,
        scrape_function=async_scrape_AlJazeera_news
    )
    
    # Convert NewsSourceResponse to RULAC_TOOL_RESULT format
    if result.total_articles == 0:
        return format_standard_tool_result(
            content="No relevant Al Jazeera News articles found for this query.",
            citations=[],
            tool_name="get_latest_news_from_AlJazeera",
            tool_params={"search_query": search_query},
            beacon_tool_source="AlJazeera"
        )
    
    # Format articles into content string
    content = f"# Al Jazeera News Articles about: {search_query}\n\n"
    for article in result.articles:
        content += f"## {article.title}\n"
        content += f"*Published: {article.date}*\n\n"
        content += f"{article.content}\n\n"
    
    # Create citations from articles
    citations = []
    for article in result.articles:
        citation = create_standard_citation(
            title=article.title,
            url="",  # URL not available in new format
            formatted_content=article.content
        )
        citations.append(citation)
    
    return format_standard_tool_result(
        content=content,
        citations=citations,
        tool_name="get_latest_news_from_AlJazeera",
        tool_params={"search_query": search_query},
        beacon_tool_source="AlJazeera"
    )

@tool
async def get_latest_news_from_Reuters(search_query: str) -> RULAC_TOOL_RESULT:
    """
    Retrieves the latest news articles from Reuters News based on the provided search query.
    Uses article snippets instead of scraping full content due to Reuters site protections.
    
    :param search_query: The search query to find relevant Reuters news articles
    :return: A dictionary with "content" containing the article content and "citations" list
    """
    result = await get_latest_news_from_source(
        search_query=search_query,
        source_name="Reuters",
        source_url="www.reuters.com/world",
        web_results_count=10,
        news_results_count=10,
        scrape_function=async_scrape_Reuters_news,
        skip_scraping_and_use_AI_summary=True
    )
    
    # Convert NewsSourceResponse to RULAC_TOOL_RESULT format
    if result.total_articles == 0:
        return format_standard_tool_result(
            content="No relevant Reuters News articles found for this query.",
            citations=[],
            tool_name="get_latest_news_from_Reuters",
            tool_params={"search_query": search_query},
            beacon_tool_source="Reuters"
        )
    
    # Format articles into content string
    content = f"# Reuters News Articles about: {search_query}\n\n"
    for article in result.articles:
        content += f"## {article.title}\n"
        content += f"*Published: {article.date}*\n\n"
        content += f"{article.content}\n\n"
    
    # Create citations from articles
    citations = []
    for article in result.articles:
        citation = create_standard_citation(
            title=article.title,
            url="",  # URL not available in new format
            formatted_content=article.content
        )
        citations.append(citation)
    
    return format_standard_tool_result(
        content=content,
        citations=citations,
        tool_name="get_latest_news_from_Reuters",
        tool_params={"search_query": search_query},
        beacon_tool_source="Reuters"
    )

@tool
async def get_latest_news_from_AP(search_query: str) -> RULAC_TOOL_RESULT:
    """
    Retrieves the latest news articles from Associated Press (AP) News based on the provided search query.
    
    :param search_query: The search query to find relevant AP news articles
    :return: A dictionary with "content" containing the article content and "citations" list
    """
    result = await get_latest_news_from_source(
        search_query=search_query,
        source_name="AP",
        source_url="apnews.com/article",
        web_results_count=1,
        news_results_count=2,
        scrape_function=async_scrape_AP_news
    )
    
    # Convert NewsSourceResponse to RULAC_TOOL_RESULT format
    if result.total_articles == 0:
        return format_standard_tool_result(
            content="No relevant AP News articles found for this query.",
            citations=[],
            tool_name="get_latest_news_from_AP",
            tool_params={"search_query": search_query},
            beacon_tool_source="AP"
        )
    
    # Format articles into content string
    content = f"# AP News Articles about: {search_query}\n\n"
    for article in result.articles:
        content += f"## {article.title}\n"
        content += f"*Published: {article.date}*\n\n"
        content += f"{article.content}\n\n"
    
    # Create citations from articles
    citations = []
    for article in result.articles:
        citation = create_standard_citation(
            title=article.title,
            url="",  # URL not available in new format
            formatted_content=article.content
        )
        citations.append(citation)
    
    return format_standard_tool_result(
        content=content,
        citations=citations,
        tool_name="get_latest_news_from_AP",
        tool_params={"search_query": search_query},
        beacon_tool_source="AP"
    )

async def save_search_results_to_cache(search_query: str, combined_content: str, all_citations: List[Dict[str, Any]]) -> None:
    """
    Save search results to a cache file.
    
    Args:
        search_query: The search query used
        combined_content: The combined content from all news sources
        all_citations: List of citations from all news sources
    """
    # Create cache directory if it doesn't exist
    cache_dir = "news_search_cache"
    os.makedirs(cache_dir, exist_ok=True)
    
    # Create a safe filename from the search query
    safe_query = re.sub(r'[^a-zA-Z0-9]', '_', search_query)
    cache_file = os.path.join(cache_dir, f"news_search_{safe_query}.md")
    citations_file = os.path.join(cache_dir, f"citations_{safe_query}.json")
    
    # Save the content and citations
    with open(cache_file, "w", encoding="utf-8") as f:
        f.write(combined_content)
    
    # Save citations to a separate JSON file
    with open(citations_file, "w", encoding="utf-8") as f:
        json.dump(all_citations, f, indent=2)
    
    log(f"Saved search results to cache: {cache_file}", "success")

async def load_latest_cached_results(search_query: str) -> Tuple[Optional[str], Optional[List[Dict[str, Any]]]]:
    """
    Load the cached search results for a given query.
    
    Args:
        search_query: The search query to find cached results for
        
    Returns:
        Tuple of (combined_content, citations) if found, (None, None) otherwise
    """
    cache_dir = "news_search_cache"
    if not os.path.exists(cache_dir):
        return None, None
    
    # Create a safe filename from the search query
    safe_query = re.sub(r'[^a-zA-Z0-9]', '_', search_query)
    cache_file = os.path.join(cache_dir, f"news_search_{safe_query}.md")
    citations_file = os.path.join(cache_dir, f"citations_{safe_query}.json")
    
    # Check if both files exist
    if not (os.path.exists(cache_file) and os.path.exists(citations_file)):
        return None, None
    
    # Load the content
    with open(cache_file, "r", encoding="utf-8") as f:
        combined_content = f.read()
    
    # Load the citations
    with open(citations_file, "r", encoding="utf-8") as f:
        citations = json.load(f)
    
    log(f"Loaded cached results from: {cache_file}", "success")
    return combined_content, citations

async def test_combined_news_search_tool(search_query: str, useCachedSearchResults: bool = False) -> None:
    """
    Test function that combines search results from BBC, Al Jazeera, and AP news sources
    and sends the combined AI summaries to Groq LLM for final analysis using ChatGroq.
    
    Args:
        search_query: The search query to find relevant news articles
        useCachedSearchResults: If True, use cached results instead of fetching new ones
    """
    # Start timing the entire process
    start_time = time.time()
    
    # Display initial message with the search query
    log(f"STARTING COMBINED NEWS SEARCH TEST: '{search_query}'", "title")
    
    # PHASE 1: CACHE HANDLING
    # Check if we should use cached results instead of making new API calls
    if useCachedSearchResults:
        # Try to load previously cached results for this query
        combined_content, all_citations = await load_latest_cached_results(search_query)
        if combined_content and all_citations:
            log("Using cached search results", "success")
            data_collection_time = 0  # No data collection time when using cache
        else:
            # Fall back to fresh search if cache retrieval fails
            log("No cached results found, falling back to fresh search", "info")
            useCachedSearchResults = False
    
    # PHASE 2: DATA COLLECTION
    # If not using cached results, perform a fresh search across all news sources
    if not useCachedSearchResults:
        # Initialize containers for the final output
        combined_content = f"# Latest news articles related to: {search_query}\n\n"
        all_citations = []
        
        # Create array to collect all article summaries from all sources
        # This allows us to sort articles by date across sources
        all_article_summaries = []
        
        try:
            # PHASE 2.1: BBC NEWS RETRIEVAL
            # Get articles from BBC News
            bbc_start_time = time.time()
            log("Fetching BBC News articles...", "info")
            bbc_result = await get_latest_news_from_BBC.ainvoke({"search_query": search_query})
            bbc_elapsed = time.time() - bbc_start_time
            
            # Log debug information about the retrieved BBC result
            log(f"DEBUG: BBC Raw Result Content Type: {type(bbc_result)}", "info")
            log(f"DEBUG: BBC Result Keys: {bbc_result.keys() if isinstance(bbc_result, dict) else 'Not a dict'}", "info")
            
            # Process BBC articles if any were found
            if isinstance(bbc_result["content"], str) and "No relevant BBC News articles found" not in bbc_result["content"]:
                # Add citations to the overall collection
                all_citations.extend(bbc_result["citations"])
                
                # Get the full content returned from BBC
                bbc_content = bbc_result["content"]
                
                # Extract individual articles from the BBC content
                # Split content by article headers (## Title)
                bbc_articles = []
                bbc_article_sections = re.split(r'##\s+', bbc_content)
                
                # Process each article section (skip the first section which is the intro)
                for section in bbc_article_sections[1:] if len(bbc_article_sections) > 1 else []:
                    # Extract title from the first line
                    title_lines = section.split('\n', 1)
                    title = title_lines[0].strip()
                    
                    # Extract date from the "*Published: date*" line
                    content = title_lines[1] if len(title_lines) > 1 else ""
                    date_match = re.search(r'\*Published:\s+(.*?)\*', content)
                    date = date_match.group(1).strip() if date_match else ""
                    
                    # Remove the date line from content to clean it up
                    if date_match:
                        content = content.replace(date_match.group(0), "").strip()
                    
                    # Add article to the collection with source information
                    all_article_summaries.append({
                        "title": title,
                        "date": date,
                        "content": content.strip(),
                        "source": "BBC News"
                    })
                
                # Log success message with count of BBC articles found
                log(f"Added {len(all_article_summaries)} BBC News articles (took {bbc_elapsed:.2f} seconds)", "success")
            else:
                # Log if no BBC articles were found
                log(f"No BBC News articles found (search took {bbc_elapsed:.2f} seconds)", "info")
                if isinstance(bbc_result["content"], str):
                    log(f"DEBUG: BBC Result Message: {bbc_result['content']}", "info")
            
            # PHASE 2.2: AL JAZEERA NEWS RETRIEVAL
            # Get articles from Al Jazeera News
            aljazeera_start_time = time.time()
            log("Fetching Al Jazeera News articles...", "info")
            aljazeera_result = await get_latest_news_from_AlJazeera.ainvoke({"search_query": search_query})
            aljazeera_elapsed = time.time() - aljazeera_start_time
            
            # Log debug information about the retrieved Al Jazeera result
            log(f"DEBUG: Al Jazeera Raw Result Content Type: {type(aljazeera_result)}", "info")
            
            # Process Al Jazeera articles if any were found
            if isinstance(aljazeera_result["content"], str) and "No relevant AlJazeera News articles found" not in aljazeera_result["content"]:
                # Add citations to the overall collection
                all_citations.extend(aljazeera_result["citations"])
                
                # Get the full content returned from Al Jazeera
                aljazeera_content = aljazeera_result["content"]
                
                # Extract individual articles from the Al Jazeera content
                # Split content by article headers (## Title)
                aljazeera_article_sections = re.split(r'##\s+', aljazeera_content)
                
                # Process each article section (skip the first section which is the intro)
                for section in aljazeera_article_sections[1:] if len(aljazeera_article_sections) > 1 else []:
                    # Extract title from the first line
                    title_lines = section.split('\n', 1)
                    title = title_lines[0].strip()
                    
                    # Extract date from the "*Published: date*" line
                    content = title_lines[1] if len(title_lines) > 1 else ""
                    date_match = re.search(r'\*Published:\s+(.*?)\*', content)
                    date = date_match.group(1).strip() if date_match else ""
                    
                    # Remove the date line from content to clean it up
                    if date_match:
                        content = content.replace(date_match.group(0), "").strip()
                    
                    # Add article to the collection with source information
                    all_article_summaries.append({
                        "title": title,
                        "date": date,
                        "content": content.strip(),
                        "source": "Al Jazeera News"
                    })
                
                # Log success message with count of Al Jazeera articles found
                # Subtract BBC citations to get only the Al Jazeera count
                log(f"Added {len(all_article_summaries) - len(bbc_result.get('citations', []))} Al Jazeera News articles (took {aljazeera_elapsed:.2f} seconds)", "success")
            else:
                # Log if no Al Jazeera articles were found
                log(f"No Al Jazeera News articles found (search took {aljazeera_elapsed:.2f} seconds)", "info")
                if isinstance(aljazeera_result["content"], str):
                    log(f"DEBUG: Al Jazeera Result Message: {aljazeera_result['content']}", "info")
            
            # PHASE 2.3: AP NEWS RETRIEVAL
            # Get articles from Associated Press News
            ap_start_time = time.time()
            log("Fetching AP News articles...", "info")
            ap_result = await get_latest_news_from_AP.ainvoke({"search_query": search_query})
            ap_elapsed = time.time() - ap_start_time
            
            # Log debug information about the retrieved AP result
            log(f"DEBUG: AP Raw Result Content Type: {type(ap_result)}", "info")
            
            # Process AP articles if any were found
            if isinstance(ap_result["content"], str) and "No relevant AP News articles found" not in ap_result["content"]:
                # Add citations to the overall collection
                all_citations.extend(ap_result["citations"])
                
                # Get the full content returned from AP
                ap_content = ap_result["content"]
                
                # Extract individual articles from the AP content
                # Split content by article headers (## Title)
                ap_article_sections = re.split(r'##\s+', ap_content)
                
                # Process each article section (skip the first section which is the intro)
                for section in ap_article_sections[1:] if len(ap_article_sections) > 1 else []:
                    # Extract title from the first line
                    title_lines = section.split('\n', 1)
                    title = title_lines[0].strip()
                    
                    # Extract date from the "*Published: date*" line
                    content = title_lines[1] if len(title_lines) > 1 else ""
                    date_match = re.search(r'\*Published:\s+(.*?)\*', content)
                    date = date_match.group(1).strip() if date_match else ""
                    
                    # Remove the date line from content to clean it up
                    if date_match:
                        content = content.replace(date_match.group(0), "").strip()
                    
                    # Add article to the collection with source information
                    all_article_summaries.append({
                        "title": title,
                        "date": date,
                        "content": content.strip(),
                        "source": "AP News"
                    })
                
                # Log success message with count of AP articles found
                # Subtract BBC and Al Jazeera citations to get only the AP count
                log(f"Added {len(all_article_summaries) - len(bbc_result.get('citations', [])) - len(aljazeera_result.get('citations', []))} AP News articles (took {ap_elapsed:.2f} seconds)", "success")
            else:
                # Log if no AP articles were found
                log(f"No AP News articles found (search took {ap_elapsed:.2f} seconds)", "info")
                if isinstance(ap_result["content"], str):
                    log(f"DEBUG: AP Result Message: {ap_result['content']}", "info")
            
            # PHASE 3: ARTICLE SORTING AND FORMATTING
            # Sort all collected articles by their publication date (oldest first)
            log("Sorting all collected articles by publication date (oldest first)...", "info")
            all_article_summaries.sort(key=lambda x: x.get("date", ""))
            
            # Build the combined content with sorted articles from all sources
            if all_article_summaries:
                log(f"Building combined content with {len(all_article_summaries)} sorted articles...", "info")
                # Create header for the combined content
                combined_content = f"# Latest news articles related to: {search_query}\n\n"
                
                # Add each article in chronological order (oldest first)
                # Include source information for each article
                for article in all_article_summaries:
                    combined_content += f"## {article['title']}\n"
                    combined_content += f"*Published: {article['date']}* | *Source: {article['source']}*\n\n"
                    combined_content += f"{article['content']}\n\n"
            
            # Log information about the final combined content
            log(f"DEBUG: Final combined content length: {len(combined_content)} chars", "info")
            log(f"DEBUG: Combined content preview: {combined_content[:1000]}...", "info")
            
            # PHASE 4: CACHE SAVING
            # Save the results to a cache file for future use
            await save_search_results_to_cache(search_query, combined_content, all_citations)
            
            # Calculate and log the total data collection time
            data_collection_time = time.time() - start_time
            log(f"Combined summaries from {len(all_citations)} total news articles (data collection took {data_collection_time:.2f} seconds)", "success")
            
        except Exception as e:
            # Log any errors that occurred during the data collection phase
            log(f"Error in combined news search: {str(e)}", "error")
            log(f"Error type: {type(e).__name__}", "error")
            import traceback
            log(f"Traceback: {traceback.format_exc()}", "error")
            return
    
    # PHASE 5: LLM ANALYSIS
    # If we have content, send it to Groq LLM for final analysis
    if all_citations:
        log("Sending combined news summaries to Groq LLM for final analysis...", "info")
        groq_start_time = time.time()
        
        # Initialize the Groq LLM with API key
        groq_api_key = "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"
        
        # Set up ChatGroq with appropriate model parameters
        chat = ChatGroq(
            groq_api_key=groq_api_key,
            # model="deepseek-r1-distill-qwen-32b",
            model="llama3-70b-8192", #only 8k token window
            # model="llama-3.1-8b-instant", #only 8k token window
            # model="llama-3.3-70b-versatile",
            temperature=0.2,  # Lower temperature for more factual outputs
            # max_tokens=1024
        )
        
        # Prepare system prompt for the LLM analysis
        # This prompt instructs the LLM on how to analyze the news articles
        system_prompt = """You are an expert news analyst.

You provide a comprehensive analysis of the current situation and the most important developments on the topic of '{search_query}'.

1. Identify common themes and developments across sources
2. Provide a clear, objective assessment of the current situation
3. Provide a list of key developments and their significance

Do not make recommendations.

Here are the news summaries to analyze:

{combined_content}
"""
        
        # Create messages for ChatGroq with the system prompt
        # Format the prompt with the search query and combined content
        messages = [
            SystemMessage(content=system_prompt.format(
                search_query=search_query,
                combined_content=combined_content
            ))
        ]
        
        # Log the full prompt being sent to the LLM
        log("\nFULL PROMPT SENT TO CHATGROQ:", "title")
        prompt_text = f"SYSTEM: {system_prompt.format(search_query=search_query, combined_content=combined_content)}"
        
        # Print the full prompt or a preview if it's very long
        if len(prompt_text) > 5000:
            # Show the full prompt regardless of length for debugging
            console.print(Panel(prompt_text, title="Complete Prompt", border_style="yellow"))
            # preview_text = prompt_text[:2000] + "\n\n[... content truncated ...]\n\n" + prompt_text[-1000:]
            # console.print(Panel(preview_text, title="Prompt Preview (truncated)", border_style="yellow"))
            log(f"Full prompt length: {len(prompt_text.split())} words", "info")
        else:
            console.print(Panel(prompt_text, title="Complete Prompt", border_style="yellow"))
        
        # Make the API call to Groq LLM
        try:
            # Start timing the API call
            response_start_time = time.time()
            # Call the LLM API
            response = chat.invoke(messages)
            # Get the response content
            ai_response = response.content
            # Calculate timing information
            groq_elapsed = time.time() - groq_start_time
            api_elapsed = time.time() - response_start_time
            
            # Display the LLM's analysis response
            log(f"\nCHATGROQ ANALYSIS (API call took {api_elapsed:.2f} seconds)", "title")
            console.print(Panel(Markdown(ai_response), title=f"Final Analysis of News Summaries", border_style="green"))
            
        except Exception as e:
            # Log any errors that occurred during the LLM API call
            groq_elapsed = time.time() - groq_start_time
            log(f"Error calling ChatGroq after {groq_elapsed:.2f} seconds: {str(e)}", "error")
            log(f"Error type: {type(e).__name__}", "error")
            import traceback
            log(f"Traceback: {traceback.format_exc()}", "error")
    else:
        # If no articles were found, log an error
        log("No news articles found to analyze", "error")
    
    # PHASE 6: FINALIZATION AND STATISTICS
    # Calculate and log the total execution time
    total_elapsed = time.time() - start_time
    log(f"COMBINED NEWS SEARCH TEST COMPLETED in {total_elapsed:.2f} seconds", "success")
    
    # Prepare timing statistics for reporting
    timing_stats = {
        "total_time": total_elapsed,
        "data_collection_time": locals().get("data_collection_time", 0),
        "groq_time": locals().get("groq_elapsed", 0),
        "sources": {
            "bbc": locals().get("bbc_elapsed", 0),
            "aljazeera": locals().get("aljazeera_elapsed", 0),
            "ap": locals().get("ap_elapsed", 0)
        }
    }
    
    # Display timing statistics in a formatted table
    from rich.table import Table
    timing_table = Table(title="Timing Statistics")
    timing_table.add_column("Component", style="cyan")
    timing_table.add_column("Time (seconds)", justify="right", style="green")
    timing_table.add_column("Percentage", justify="right", style="yellow")
    
    # Add rows for each source
    timing_table.add_row("BBC News", f"{timing_stats['sources']['bbc']:.2f}", 
                         f"{(timing_stats['sources']['bbc']/total_elapsed*100):.1f}%")
    timing_table.add_row("Al Jazeera News", f"{timing_stats['sources']['aljazeera']:.2f}", 
                         f"{(timing_stats['sources']['aljazeera']/total_elapsed*100):.1f}%")
    timing_table.add_row("AP News", f"{timing_stats['sources']['ap']:.2f}", 
                         f"{(timing_stats['sources']['ap']/total_elapsed*100):.1f}%")
    
    # Add Groq time if available
    if "groq_elapsed" in locals():
        timing_table.add_row("Groq LLM", f"{timing_stats['groq_time']:.2f}", 
                             f"{(timing_stats['groq_time']/total_elapsed*100):.1f}%")
    
    # Add total time
    timing_table.add_row("Total Execution", f"{total_elapsed:.2f}", "100.0%", style="bold")
    
    # Display the table
    console.print(timing_table)

@tool
async def get_combined_news(search_query: str) -> RULAC_TOOL_RESULT:
    """
    Retrieves and combines news articles from multiple sources (BBC, Al Jazeera, AP) based on the provided search query.
    Returns a combined summary of news articles without LLM analysis.
    
    :param search_query: The search query to find relevant news articles
    :return: A dictionary with "content" containing the combined article content and "citations" list
    """
    # Start timing the entire process
    start_time = time.time()
    
    # Display initial message with the search query
    log(f"STARTING COMBINED NEWS SEARCH: '{search_query}'", "title")
    
    # Initialize containers for the final output
    combined_content = f"# Latest news articles related to: {search_query}\n\n"
    all_citations = []
    
    # Create array to collect all article summaries from all sources
    all_article_summaries = []
    
    try:
        # PHASE 1: BBC NEWS RETRIEVAL
        # Get articles from BBC News
        bbc_start_time = time.time()
        log("Fetching BBC News articles...", "info")
        bbc_result = await get_latest_news_from_BBC.ainvoke({"search_query": search_query})
        bbc_elapsed = time.time() - bbc_start_time
        
        # Process BBC articles if any were found
        if isinstance(bbc_result["content"], str) and "No relevant BBC News articles found" not in bbc_result["content"]:
            # Add citations to the overall collection
            all_citations.extend(bbc_result["citations"])
            
            # Get the full content returned from BBC
            bbc_content = bbc_result["content"]
            
            # Extract individual articles from the BBC content
            # Split content by article headers (## Title)
            bbc_article_sections = re.split(r'##\s+', bbc_content)
            
            # Process each article section (skip the first section which is the intro)
            for section in bbc_article_sections[1:] if len(bbc_article_sections) > 1 else []:
                # Extract title from the first line
                title_lines = section.split('\n', 1)
                title = title_lines[0].strip()
                
                # Extract date from the "*Published: date*" line
                content = title_lines[1] if len(title_lines) > 1 else ""
                date_match = re.search(r'\*Published:\s+(.*?)\*', content)
                date = date_match.group(1).strip() if date_match else ""
                
                # Remove the date line from content to clean it up
                if date_match:
                    content = content.replace(date_match.group(0), "").strip()
                
                # Add article to the collection with source information
                all_article_summaries.append({
                    "title": title,
                    "date": date,
                    "content": content.strip(),
                    "source": "BBC News"
                })
            
            # Log success message with count of BBC articles found
            log(f"Added {len(all_article_summaries)} BBC News articles (took {bbc_elapsed:.2f} seconds)", "success")
        else:
            # Log if no BBC articles were found
            log(f"No BBC News articles found (search took {bbc_elapsed:.2f} seconds)", "info")
        
        # PHASE 2: AL JAZEERA NEWS RETRIEVAL
        # Get articles from Al Jazeera News
        aljazeera_start_time = time.time()
        log("Fetching Al Jazeera News articles...", "info")
        aljazeera_result = await get_latest_news_from_AlJazeera.ainvoke({"search_query": search_query})
        aljazeera_elapsed = time.time() - aljazeera_start_time
        
        # Process Al Jazeera articles if any were found
        if isinstance(aljazeera_result["content"], str) and "No relevant AlJazeera News articles found" not in aljazeera_result["content"]:
            # Add citations to the overall collection
            all_citations.extend(aljazeera_result["citations"])
            
            # Get the full content returned from Al Jazeera
            aljazeera_content = aljazeera_result["content"]
            
            # Extract individual articles from the Al Jazeera content
            # Split content by article headers (## Title)
            aljazeera_article_sections = re.split(r'##\s+', aljazeera_content)
            
            # Process each article section (skip the first section which is the intro)
            for section in aljazeera_article_sections[1:] if len(aljazeera_article_sections) > 1 else []:
                # Extract title from the first line
                title_lines = section.split('\n', 1)
                title = title_lines[0].strip()
                
                # Extract date from the "*Published: date*" line
                content = title_lines[1] if len(title_lines) > 1 else ""
                date_match = re.search(r'\*Published:\s+(.*?)\*', content)
                date = date_match.group(1).strip() if date_match else ""
                
                # Remove the date line from content to clean it up
                if date_match:
                    content = content.replace(date_match.group(0), "").strip()
                
                # Add article to the collection with source information
                all_article_summaries.append({
                    "title": title,
                    "date": date,
                    "content": content.strip(),
                    "source": "Al Jazeera News"
                })
            
            # Log success message with count of Al Jazeera articles found
            # Subtract BBC citations to get only the Al Jazeera count
            log(f"Added {len(all_article_summaries) - len(bbc_result.get('citations', []))} Al Jazeera News articles (took {aljazeera_elapsed:.2f} seconds)", "success")
        else:
            # Log if no Al Jazeera articles were found
            log(f"No Al Jazeera News articles found (search took {aljazeera_elapsed:.2f} seconds)", "info")
        
        # PHASE 3: AP NEWS RETRIEVAL
        # Get articles from Associated Press News
        ap_start_time = time.time()
        log("Fetching AP News articles...", "info")
        ap_result = await get_latest_news_from_AP.ainvoke({"search_query": search_query})
        ap_elapsed = time.time() - ap_start_time
        
        # Process AP articles if any were found
        if isinstance(ap_result["content"], str) and "No relevant AP News articles found" not in ap_result["content"]:
            # Add citations to the overall collection
            all_citations.extend(ap_result["citations"])
            
            # Get the full content returned from AP
            ap_content = ap_result["content"]
            
            # Extract individual articles from the AP content
            # Split content by article headers (## Title)
            ap_article_sections = re.split(r'##\s+', ap_content)
            
            # Process each article section (skip the first section which is the intro)
            for section in ap_article_sections[1:] if len(ap_article_sections) > 1 else []:
                # Extract title from the first line
                title_lines = section.split('\n', 1)
                title = title_lines[0].strip()
                
                # Extract date from the "*Published: date*" line
                content = title_lines[1] if len(title_lines) > 1 else ""
                date_match = re.search(r'\*Published:\s+(.*?)\*', content)
                date = date_match.group(1).strip() if date_match else ""
                
                # Remove the date line from content to clean it up
                if date_match:
                    content = content.replace(date_match.group(0), "").strip()
                
                # Add article to the collection with source information
                all_article_summaries.append({
                    "title": title,
                    "date": date,
                    "content": content.strip(),
                    "source": "AP News"
                })
            
            # Log success message with count of AP articles found
            # Subtract BBC and Al Jazeera citations to get only the AP count
            log(f"Added {len(all_article_summaries) - len(bbc_result.get('citations', [])) - len(aljazeera_result.get('citations', []))} AP News articles (took {ap_elapsed:.2f} seconds)", "success")
        else:
            # Log if no AP articles were found
            log(f"No AP News articles found (search took {ap_elapsed:.2f} seconds)", "info")
        
        # PHASE 4: ARTICLE SORTING AND FORMATTING
        # Sort all collected articles by their publication date (oldest first)
        log("Sorting all collected articles by publication date (oldest first)...", "info")
        all_article_summaries.sort(key=lambda x: x.get("date", ""))
        
        # Build the combined content with sorted articles from all sources
        if all_article_summaries:
            log(f"Building combined content with {len(all_article_summaries)} sorted articles...", "info")
            # Create header for the combined content
            combined_content = f"# Latest news articles related to: {search_query}\n\n"
            
            # Add each article in chronological order (oldest first)
            # Include source information for each article
            for article in all_article_summaries:
                combined_content += f"## {article['title']}\n"
                combined_content += f"*Published: {article['date']}* | *Source: {article['source']}*\n\n"
                combined_content += f"{article['content']}\n\n"
        
        # Calculate and log the total data collection time
        data_collection_time = time.time() - start_time
        log(f"Combined {len(all_citations)} total news articles (took {data_collection_time:.2f} seconds)", "success")
        
        # Display formatted results
        display_formatted_results(
            cleaned_tool_message=combined_content,
            title=f"COMBINED NEWS ARTICLES",
            tool_name="get_combined_news",
            tool_params={"search_query": search_query},
            citations=all_citations,
            beacon_tool_source="CombinedNews",
            showFull=True
            )

        # Return the final RULAC_TOOL_RESULT
        if not all_citations:
            # If no articles were found, return appropriate message
            log("No news articles found", "error")
            return format_standard_tool_result(
                content="No relevant news articles found for this query.",
                citations=[],
                tool_name="get_combined_news",
                tool_params={"search_query": search_query},
                beacon_tool_source="CombinedNews"
            )
        
        # Return the combined content and citations
        return format_standard_tool_result(
            content=combined_content,
            citations=all_citations,
            tool_name="get_combined_news",
            tool_params={"search_query": search_query},
            beacon_tool_source="CombinedNews",
        )
        
    except Exception as e:
        # Log any errors that occurred during the process
        error_message = f"Error in combined news search: {str(e)}"
        log(error_message, "error")
        log(f"Error type: {type(e).__name__}", "error")
        import traceback
        log(f"Traceback: {traceback.format_exc()}", "error")
        
        # Return error message as content
        return format_standard_tool_result(
            content=f"Error retrieving news articles: {str(e)}",
            citations=[],
            tool_name="get_combined_news",
            tool_params={"search_query": search_query},
            beacon_tool_source="CombinedNews"
        )

if __name__ == "__main__":
    import asyncio
    
    async def run_tests():
        """Run all test functions"""
        log("STARTING NEWS TOOLS TESTS", "title")
        
        # Test scenarios for news sources
        news_test_scenarios = [
            # {
            #     "name": "Get News about deepseek",
            #     "params": {
            #         "search_query": "deepseek"
            #     },
            # },
            # {
            #     "name": "Get News about Technology",
            #     "params": {
            #         "search_query": "artificial intelligence drone"
            #     },
            # },
            # {
            #     "name": "Get News about Ukraine War",
            #     "params": {
            #         "search_query": "ukraine ceasefire"
            #     },
            # },
            {
                "name": "Get News about Ukraine conflict",
                "params": {
                    "search_query": "ukraine conflict"
                },
            },
        ]

        # # Use the standardized test framework
        # await standardized_tool_test(
        #     tool_function=get_latest_news_from_BBC,
        #     test_scenarios=news_test_scenarios,
        #     test_title="BBC News Retrieval Tool Test"
        # )

        await standardized_tool_test(
            tool_function=get_latest_news_from_AlJazeera,
            test_scenarios=news_test_scenarios,
            test_title="Al Jazeera News Retrieval Tool Test"
        )

        # # await standardized_tool_test(
        # #     tool_function=get_latest_news_from_Reuters,
        # #     test_scenarios=news_test_scenarios,
        # #     test_title="Reuters News Retrieval Tool Test"
        # # )

        # await standardized_tool_test(
        #     tool_function=get_latest_news_from_AP,
        #     test_scenarios=news_test_scenarios,
        #     test_title="AP News Retrieval Tool Test"
        # )
        
        # Test the combined news search tool
        # await test_combined_news_search_tool("trump and putin negotiations", useCachedSearchResults=True)
        # await test_combined_news_search_tool("baldoni and blake lively", useCachedSearchResults=True)
        # await test_combined_news_search_tool("ukraine conflict", useCachedSearchResults=True)
        # await test_combined_news_search_tool("ukraine ceasefire", useCachedSearchResults=False)
        
        # Test the new get_combined_news tool
        # await standardized_tool_test(
        #     tool_function=get_combined_news,
        #     test_scenarios=[
        #         {
        #             "name": "Get Combined News about Ukraine Conflict",
        #             "params": {
        #                 "search_query": "ukraine conflict"
        #             }
        #         }
        #     ],
        #     test_title="Combined News Retrieval Tool Test"
        # )
       
        # await test_combined_news_search_tool("ukraine conflict classification under IHL law", useCachedSearchResults=False)

        log("ALL TESTS COMPLETED", "success")
    
    # Run the async test function
    asyncio.run(run_tests()) 


